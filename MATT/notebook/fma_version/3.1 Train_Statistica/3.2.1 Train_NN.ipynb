{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "import sys\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "\"\"\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "\n",
    "import platform\n",
    "if 'Windows' in platform.platform():\n",
    "    ROOT_PATH = \"D:/PycharmProjects/HMAN\"\n",
    "else:\n",
    "    ROOT_PATH = \"/home/xkliu/PycharmProjects/HMAN\"\n",
    "RAW_DATA_PATH = ROOT_PATH  + \"/raw_data\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "os.chdir(ROOT_PATH)\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "from kddirkit.utils import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from kddirkit.networks.encoders import SentenceEncoder\n",
    "from kddirkit.networks.models import BaselineModel\n",
    "from kddirkit.config import *\n",
    "# from kddirkit.dataloaders import LoadNYT, LoadHierData\n",
    "from kddirkit.frameworks import Trainer\n",
    "from kddirkit.losses.FocalLoss import FocalLoss\n",
    "\n",
    "project_name = 'HMAN'\n",
    "\n",
    "logger = logging.getLogger(project_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "track_dtype = {'track_id': int, 'album_id': int, 'album_type': str, 'artist_id': int, 'set_split': str,\n",
    "               'set_subset': str, 'track_genre_top': str, 'track_genres': str, 'track_genres_all': str,\n",
    "               'track_title': str}\n",
    "genres_converters = {'track_genres': literal_eval, 'track_genres_all': literal_eval}\n",
    "medium_data = pd.read_csv(RAW_DATA_PATH + '/medium_data.csv', converters=genres_converters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "medium_data_train = pd.read_csv(RAW_DATA_PATH + '/medium_data_train.csv', converters=genres_converters)\n",
    "medium_data_test = pd.read_csv(RAW_DATA_PATH + '/medium_data_test.csv', converters=genres_converters)\n",
    "medium_data_val = pd.read_csv(RAW_DATA_PATH + '/medium_data_val.csv', converters=genres_converters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "medium_instance_triple = np.load(DATA_PATH + '/' + 'medium_instance_triple.npy')\n",
    "medium_instance_scope = np.load(DATA_PATH + '/' + 'medium_instance_scope.npy')\n",
    "medium_label = np.load(DATA_PATH + '/' + 'medium_label.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "medium_train_instance_triple = np.load(DATA_PATH + '/' + 'medium_train_instance_triple.npy')\n",
    "medium_train_instance_scope = np.load(DATA_PATH + '/' + 'medium_train_instance_scope.npy')\n",
    "medium_train_label = np.load(DATA_PATH + '/' + 'medium_train_label.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "medium_val_instance_triple = np.load(DATA_PATH + '/' + 'medium_val_instance_triple.npy')\n",
    "medium_val_instance_scope = np.load(DATA_PATH + '/' + 'medium_val_instance_scope.npy')\n",
    "medium_val_label = np.load(DATA_PATH + '/' + 'medium_val_label.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "medium_test_entity_pair = np.load(DATA_PATH + '/' + 'medium_test_entity_pair.npy')\n",
    "medium_test_entity_scope = np.load(DATA_PATH + '/' + 'medium_test_entity_scope.npy')\n",
    "medium_test_label = np.load(DATA_PATH + '/' + 'medium_test_label.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "medium_label_transform = np.load(DATA_PATH + '/' + 'medium_label_transform.npy')\n",
    "medium_train_label_transform = np.load(DATA_PATH + '/' + 'medium_train_label_transform.npy')\n",
    "medium_val_label_transform = np.load(DATA_PATH + '/' + 'medium_val_label_transform.npy')\n",
    "medium_test_label_transform = np.load(DATA_PATH + '/' + 'medium_test_label_transform.npy')\n",
    "medium_label_bottom_transform = np.load(DATA_PATH + '/' + 'medium_label_bottom_transform.npy')\n",
    "medium_train_label_bottom_transform = np.load(DATA_PATH + '/' + 'medium_train_label_bottom_transform.npy')\n",
    "medium_val_label_bottom_transform = np.load(DATA_PATH + '/' + 'medium_val_label_bottom_transform.npy')\n",
    "medium_test_label_bottom_transform = np.load(DATA_PATH + '/' + 'medium_test_label_bottom_transform.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\AppData\\Local\\Temp\\ipykernel_29588\\691608824.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  medium_data_train_sort = pd.read_csv(RAW_DATA_PATH + '/' + 'medium_data_train_sort.txt', sep ='-----',  skiprows =1, names  = col_name)\n",
      "C:\\Users\\Johan\\AppData\\Local\\Temp\\ipykernel_29588\\691608824.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  medium_data_val_sort = pd.read_csv(RAW_DATA_PATH + '/' + 'medium_data_VAL_sort.txt', sep = '-----',  skiprows =1, names  = col_name)\n",
      "C:\\Users\\Johan\\AppData\\Local\\Temp\\ipykernel_29588\\691608824.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  medium_data_test_sort = pd.read_csv(RAW_DATA_PATH + '/' + 'medium_data_test_sort.txt', sep = '-----', skiprows =1, names  = col_name)\n"
     ]
    }
   ],
   "source": [
    "col_name = ['track_id', 'album_id', 'album_type', 'artist_id', 'set_split', 'set_subset', 'track_genres_top', 'track_genre', 'track_genres_all']\n",
    "medium_data_train_sort = pd.read_csv(RAW_DATA_PATH + '/' + 'medium_data_train_sort.txt', sep ='-----',  skiprows =1, names  = col_name)\n",
    "medium_data_val_sort = pd.read_csv(RAW_DATA_PATH + '/' + 'medium_data_VAL_sort.txt', sep = '-----',  skiprows =1, names  = col_name)\n",
    "medium_data_test_sort = pd.read_csv(RAW_DATA_PATH + '/' + 'medium_data_test_sort.txt', sep = '-----', skiprows =1, names  = col_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "       track_id  album_id        album_type  artist_id set_split set_subset  \\\n0             2         1             Album          1  training      small   \n1             5         1             Album          1  training      small   \n2            10         6             Album          6  training      small   \n3           140        61             Album         54  training      small   \n4           141        60             Album         54  training      small   \n...         ...       ...               ...        ...       ...        ...   \n24995    155297     22935             Album      24354  training     medium   \n24996    155298     22936             Album      22050  training     medium   \n24997    155306     22936             Album      22050  training     medium   \n24998    155307     22937  Live Performance       7820  training     medium   \n24999    155314     22940  Live Performance      24357  training     medium   \n\n      track_genre_top     track_genres track_genres_all         track_title  \n0             Hip-Hop             [21]             [21]                Food  \n1             Hip-Hop             [21]             [21]          This World  \n2                 Pop             [10]             [10]             Freeway  \n3                Folk             [17]             [17]  Queen Of The Wires  \n4                Folk             [17]             [17]                Ohio  \n...               ...              ...              ...                 ...  \n24995    Instrumental  [18, 107, 1235]  [107, 18, 1235]       Nebula Reborn  \n24996            Folk        [17, 103]        [17, 103]     An Idiot Abroad  \n24997            Folk        [17, 103]        [17, 103]            Tiny Man  \n24998    Experimental              [1]          [1, 38]               Kolka  \n24999            Rock             [25]         [25, 12]        Miracle Grow  \n\n[25000 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>album_id</th>\n      <th>album_type</th>\n      <th>artist_id</th>\n      <th>set_split</th>\n      <th>set_subset</th>\n      <th>track_genre_top</th>\n      <th>track_genres</th>\n      <th>track_genres_all</th>\n      <th>track_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Album</td>\n      <td>1</td>\n      <td>training</td>\n      <td>small</td>\n      <td>Hip-Hop</td>\n      <td>[21]</td>\n      <td>[21]</td>\n      <td>Food</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Album</td>\n      <td>1</td>\n      <td>training</td>\n      <td>small</td>\n      <td>Hip-Hop</td>\n      <td>[21]</td>\n      <td>[21]</td>\n      <td>This World</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>6</td>\n      <td>Album</td>\n      <td>6</td>\n      <td>training</td>\n      <td>small</td>\n      <td>Pop</td>\n      <td>[10]</td>\n      <td>[10]</td>\n      <td>Freeway</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>140</td>\n      <td>61</td>\n      <td>Album</td>\n      <td>54</td>\n      <td>training</td>\n      <td>small</td>\n      <td>Folk</td>\n      <td>[17]</td>\n      <td>[17]</td>\n      <td>Queen Of The Wires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>141</td>\n      <td>60</td>\n      <td>Album</td>\n      <td>54</td>\n      <td>training</td>\n      <td>small</td>\n      <td>Folk</td>\n      <td>[17]</td>\n      <td>[17]</td>\n      <td>Ohio</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24995</th>\n      <td>155297</td>\n      <td>22935</td>\n      <td>Album</td>\n      <td>24354</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Instrumental</td>\n      <td>[18, 107, 1235]</td>\n      <td>[107, 18, 1235]</td>\n      <td>Nebula Reborn</td>\n    </tr>\n    <tr>\n      <th>24996</th>\n      <td>155298</td>\n      <td>22936</td>\n      <td>Album</td>\n      <td>22050</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Folk</td>\n      <td>[17, 103]</td>\n      <td>[17, 103]</td>\n      <td>An Idiot Abroad</td>\n    </tr>\n    <tr>\n      <th>24997</th>\n      <td>155306</td>\n      <td>22936</td>\n      <td>Album</td>\n      <td>22050</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Folk</td>\n      <td>[17, 103]</td>\n      <td>[17, 103]</td>\n      <td>Tiny Man</td>\n    </tr>\n    <tr>\n      <th>24998</th>\n      <td>155307</td>\n      <td>22937</td>\n      <td>Live Performance</td>\n      <td>7820</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Experimental</td>\n      <td>[1]</td>\n      <td>[1, 38]</td>\n      <td>Kolka</td>\n    </tr>\n    <tr>\n      <th>24999</th>\n      <td>155314</td>\n      <td>22940</td>\n      <td>Live Performance</td>\n      <td>24357</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Rock</td>\n      <td>[25]</td>\n      <td>[25, 12]</td>\n      <td>Miracle Grow</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "       track_id  album_id        album_type  artist_id set_split set_subset  \\\n0             2         1             Album          1  training      small   \n1             5         1             Album          1  training      small   \n2             3         1             Album          1  training     medium   \n3           134         1             Album          1  training     medium   \n4         10666         1             Album          1  training     medium   \n...         ...       ...               ...        ...       ...        ...   \n19917    155297     22935             Album      24354  training     medium   \n19918    155298     22936             Album      22050  training     medium   \n19919    155306     22936             Album      22050  training     medium   \n19920    155307     22937  Live Performance       7820  training     medium   \n19921    155314     22940  Live Performance      24357  training     medium   \n\n      track_genres_top      track_genre track_genres_all  \n0              Hip-Hop             [21]             [21]  \n1              Hip-Hop             [21]             [21]  \n2              Hip-Hop             [21]             [21]  \n3              Hip-Hop             [21]             [21]  \n4              Hip-Hop             [21]             [21]  \n...                ...              ...              ...  \n19917     Instrumental  [18, 107, 1235]  [107, 18, 1235]  \n19918             Folk        [17, 103]        [17, 103]  \n19919             Folk        [17, 103]        [17, 103]  \n19920     Experimental              [1]          [1, 38]  \n19921             Rock             [25]         [25, 12]  \n\n[19922 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>album_id</th>\n      <th>album_type</th>\n      <th>artist_id</th>\n      <th>set_split</th>\n      <th>set_subset</th>\n      <th>track_genres_top</th>\n      <th>track_genre</th>\n      <th>track_genres_all</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Album</td>\n      <td>1</td>\n      <td>training</td>\n      <td>small</td>\n      <td>Hip-Hop</td>\n      <td>[21]</td>\n      <td>[21]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Album</td>\n      <td>1</td>\n      <td>training</td>\n      <td>small</td>\n      <td>Hip-Hop</td>\n      <td>[21]</td>\n      <td>[21]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Album</td>\n      <td>1</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Hip-Hop</td>\n      <td>[21]</td>\n      <td>[21]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>134</td>\n      <td>1</td>\n      <td>Album</td>\n      <td>1</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Hip-Hop</td>\n      <td>[21]</td>\n      <td>[21]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10666</td>\n      <td>1</td>\n      <td>Album</td>\n      <td>1</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Hip-Hop</td>\n      <td>[21]</td>\n      <td>[21]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19917</th>\n      <td>155297</td>\n      <td>22935</td>\n      <td>Album</td>\n      <td>24354</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Instrumental</td>\n      <td>[18, 107, 1235]</td>\n      <td>[107, 18, 1235]</td>\n    </tr>\n    <tr>\n      <th>19918</th>\n      <td>155298</td>\n      <td>22936</td>\n      <td>Album</td>\n      <td>22050</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Folk</td>\n      <td>[17, 103]</td>\n      <td>[17, 103]</td>\n    </tr>\n    <tr>\n      <th>19919</th>\n      <td>155306</td>\n      <td>22936</td>\n      <td>Album</td>\n      <td>22050</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Folk</td>\n      <td>[17, 103]</td>\n      <td>[17, 103]</td>\n    </tr>\n    <tr>\n      <th>19920</th>\n      <td>155307</td>\n      <td>22937</td>\n      <td>Live Performance</td>\n      <td>7820</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Experimental</td>\n      <td>[1]</td>\n      <td>[1, 38]</td>\n    </tr>\n    <tr>\n      <th>19921</th>\n      <td>155314</td>\n      <td>22940</td>\n      <td>Live Performance</td>\n      <td>24357</td>\n      <td>training</td>\n      <td>medium</td>\n      <td>Rock</td>\n      <td>[25]</td>\n      <td>[25, 12]</td>\n    </tr>\n  </tbody>\n</table>\n<p>19922 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_data_train_sort"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Load metadata and features.\n",
    "tracks = utils.load(RAW_DATA_PATH + '/fma_metadata/tracks.csv')\n",
    "genres = utils.load(RAW_DATA_PATH + '/fma_metadata/genres.csv')\n",
    "features = utils.load(RAW_DATA_PATH + '/fma_metadata/features.csv')\n",
    "echonest = utils.load(RAW_DATA_PATH + '/fma_metadata/echonest.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "0             2\n1             5\n2             3\n3           134\n4         10666\n          ...  \n19917    155297\n19918    155298\n19919    155306\n19920    155307\n19921    155314\nName: track_id, Length: 19922, dtype: int64"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_data_train_sort.track_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19922 training examples, 2573 testing examples\n",
      "84 features, 16 classes\n"
     ]
    }
   ],
   "source": [
    "small = tracks['set', 'subset'] <= 'medium'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "small = tracks['set', 'subset'] <= 'medium'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "val = tracks['set', 'split'] == 'validation'\n",
    "test = tracks['set', 'split'] == 'test'\n",
    "\n",
    "y_train = tracks.loc[medium_data_train_sort.track_id, ('track', 'genre_top')]\n",
    "y_val = tracks.loc[medium_data_val_sort.track_id, ('track', 'genre_top')]\n",
    "y_test = tracks.loc[medium_data_test_sort.track_id, ('track', 'genre_top')]\n",
    "X_train = features.loc[medium_data_train_sort.track_id, 'chroma_cens']\n",
    "X_val= features.loc[medium_data_val_sort.track_id, 'chroma_cens']\n",
    "X_test = features.loc[medium_data_test_sort.track_id, 'chroma_cens']\n",
    "\n",
    "print('{} training examples, {} testing examples'.format(y_train.size, y_test.size))\n",
    "print('{} features, {} classes'.format(X_train.shape[1], np.unique(y_train).size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.3177227 ,  0.47041556,  0.83269   , ...,  0.12363743,\n         0.11432877,  0.13681701],\n       [ 0.44840875,  0.2697352 ,  0.734159  , ...,  0.08551046,\n         0.10056883,  0.12318285],\n       [-0.18923731, -0.43042916, -0.58131   , ...,  0.19250631,\n         0.10459377,  0.16793416],\n       ...,\n       [-0.05861488,  0.04176554,  0.16710621, ...,  0.11239246,\n         0.10370939,  0.08271191],\n       [ 0.751266  ,  3.2632694 ,  2.6064632 , ...,  0.133206  ,\n         0.0833592 ,  0.13430515],\n       [-0.33007073, -0.7555196 , -1.0224332 , ...,  0.14513223,\n         0.13972357,  0.1569068 ]], dtype=float32)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Be sure training samples are shuffled.\n",
    "X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "\n",
    "X_train_np = np.array(X_train).astype('float32')\n",
    "X_test_np = np.array(X_test).astype('float32')\n",
    "X_val_np = np.array(X_val).astype('float32')\n",
    "\n",
    "y_train_np = np.argmax(pd.get_dummies(y_train).to_numpy(), axis=1)\n",
    "y_test_np = np.argmax(pd.get_dummies(y_test).to_numpy(), axis = 1)\n",
    "y_val_np = np.argmax(pd.get_dummies(y_val).to_numpy(), axis = 1)\n",
    "X_train_np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\anaconda3\\envs\\HMAN\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Johan\\anaconda3\\envs\\HMAN\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Johan\\anaconda3\\envs\\HMAN\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['tuple']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "statistics  kurtosis                                                    \\\nnumber            01        02        03        04        05        06   \ntrack_id                                                                 \n123369     -0.306721  0.279311  0.113560 -0.076699 -0.214043  0.086277   \n114240      0.188264  0.145521  0.094648  1.678559  1.544423  0.761664   \n75304      -0.223709 -0.321264 -0.157837 -0.046350 -0.207695 -0.020741   \n147188      0.905813 -0.279944  0.439759 -0.102341 -0.697381 -0.311867   \n17498      -0.875098 -0.469244 -0.277808 -0.281861 -0.404335 -0.190690   \n...              ...       ...       ...       ...       ...       ...   \n12425      -0.857010 -0.925512  0.251688 -0.164870 -0.199754  0.355163   \n18804       0.620468 -0.189069 -0.213413  0.577919 -0.091439 -0.431539   \n95248      -0.139316 -0.006462 -0.014189  0.195926  0.074067 -0.406737   \n17587       0.383935  2.141251  0.454010 -0.241576 -0.262764  0.668187   \n70936      -0.314699 -0.537996 -0.242504 -0.259642 -0.648286 -0.538052   \n\nstatistics                                          ...       std            \\\nnumber            07        08        09        10  ...        03        04   \ntrack_id                                            ...                       \n123369     -0.258529  0.315039 -0.227108 -0.370168  ... -0.320647 -0.728734   \n114240      2.015886  0.002172 -0.036250 -0.006739  ... -0.640900 -0.775902   \n75304       1.250639  2.359803 -0.421057 -0.603971  ...  0.552780 -1.032382   \n147188      0.457256  2.678123 -0.134226  1.366826  ...  0.684057  0.400656   \n17498      -0.472516 -0.531565 -0.088520 -0.282635  ...  0.342480 -0.306736   \n...              ...       ...       ...       ...  ...       ...       ...   \n12425      -0.626647  0.292643  0.462942  1.609487  ...  0.527872 -0.373559   \n18804      -0.409015  0.011714 -0.253144 -0.052158  ... -1.190409 -1.562879   \n95248      -0.729983 -0.191057 -0.535689 -0.268353  ... -0.808541  0.099055   \n17587       0.744732  1.478032  0.822140 -0.563817  ... -1.697793 -0.665030   \n70936      -0.559009 -0.371742 -0.542160  0.329054  ...  0.967511  1.325316   \n\nstatistics                                                              \\\nnumber            05        06        07        08        09        10   \ntrack_id                                                                 \n123369      0.349349 -0.122466 -0.106398 -0.371939 -0.283395 -0.107322   \n114240     -0.519872 -1.006744 -0.360873 -0.985945 -0.120111 -1.102508   \n75304      -0.204878 -0.779071 -1.558045 -1.011316 -0.028531  1.690285   \n147188      2.129617  0.643447 -0.236078  0.472530  0.201846  0.717411   \n17498      -0.397212 -0.238033  1.597515  1.560039  0.970806  2.351777   \n...              ...       ...       ...       ...       ...       ...   \n12425       0.006812  0.130743 -0.344425 -0.830342 -1.115266 -1.616840   \n18804      -1.072920 -1.005408 -1.346681 -1.646640 -1.393278 -1.580012   \n95248      -1.027515 -0.075408  1.277822 -0.745751  0.775518 -0.400837   \n17587      -0.196496 -0.881531 -0.731217 -1.206747 -0.750349  0.142435   \n70936       1.732603  0.463518  1.145644  0.658337  1.021091  0.453733   \n\nstatistics                      \nnumber            11        12  \ntrack_id                        \n123369     -0.147333  0.398578  \n114240     -0.540157  0.014309  \n75304      -0.425251  1.275594  \n147188      0.133039  0.387491  \n17498       0.917199 -0.685662  \n...              ...       ...  \n12425      -0.097982  0.934535  \n18804      -1.496626 -1.511881  \n95248      -0.450499 -1.126337  \n17587      -1.031463  0.327783  \n70936       0.577647  0.964795  \n\n[19922 rows x 84 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>statistics</th>\n      <th colspan=\"10\" halign=\"left\">kurtosis</th>\n      <th>...</th>\n      <th colspan=\"10\" halign=\"left\">std</th>\n    </tr>\n    <tr>\n      <th>number</th>\n      <th>01</th>\n      <th>02</th>\n      <th>03</th>\n      <th>04</th>\n      <th>05</th>\n      <th>06</th>\n      <th>07</th>\n      <th>08</th>\n      <th>09</th>\n      <th>10</th>\n      <th>...</th>\n      <th>03</th>\n      <th>04</th>\n      <th>05</th>\n      <th>06</th>\n      <th>07</th>\n      <th>08</th>\n      <th>09</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n    </tr>\n    <tr>\n      <th>track_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>123369</th>\n      <td>-0.306721</td>\n      <td>0.279311</td>\n      <td>0.113560</td>\n      <td>-0.076699</td>\n      <td>-0.214043</td>\n      <td>0.086277</td>\n      <td>-0.258529</td>\n      <td>0.315039</td>\n      <td>-0.227108</td>\n      <td>-0.370168</td>\n      <td>...</td>\n      <td>-0.320647</td>\n      <td>-0.728734</td>\n      <td>0.349349</td>\n      <td>-0.122466</td>\n      <td>-0.106398</td>\n      <td>-0.371939</td>\n      <td>-0.283395</td>\n      <td>-0.107322</td>\n      <td>-0.147333</td>\n      <td>0.398578</td>\n    </tr>\n    <tr>\n      <th>114240</th>\n      <td>0.188264</td>\n      <td>0.145521</td>\n      <td>0.094648</td>\n      <td>1.678559</td>\n      <td>1.544423</td>\n      <td>0.761664</td>\n      <td>2.015886</td>\n      <td>0.002172</td>\n      <td>-0.036250</td>\n      <td>-0.006739</td>\n      <td>...</td>\n      <td>-0.640900</td>\n      <td>-0.775902</td>\n      <td>-0.519872</td>\n      <td>-1.006744</td>\n      <td>-0.360873</td>\n      <td>-0.985945</td>\n      <td>-0.120111</td>\n      <td>-1.102508</td>\n      <td>-0.540157</td>\n      <td>0.014309</td>\n    </tr>\n    <tr>\n      <th>75304</th>\n      <td>-0.223709</td>\n      <td>-0.321264</td>\n      <td>-0.157837</td>\n      <td>-0.046350</td>\n      <td>-0.207695</td>\n      <td>-0.020741</td>\n      <td>1.250639</td>\n      <td>2.359803</td>\n      <td>-0.421057</td>\n      <td>-0.603971</td>\n      <td>...</td>\n      <td>0.552780</td>\n      <td>-1.032382</td>\n      <td>-0.204878</td>\n      <td>-0.779071</td>\n      <td>-1.558045</td>\n      <td>-1.011316</td>\n      <td>-0.028531</td>\n      <td>1.690285</td>\n      <td>-0.425251</td>\n      <td>1.275594</td>\n    </tr>\n    <tr>\n      <th>147188</th>\n      <td>0.905813</td>\n      <td>-0.279944</td>\n      <td>0.439759</td>\n      <td>-0.102341</td>\n      <td>-0.697381</td>\n      <td>-0.311867</td>\n      <td>0.457256</td>\n      <td>2.678123</td>\n      <td>-0.134226</td>\n      <td>1.366826</td>\n      <td>...</td>\n      <td>0.684057</td>\n      <td>0.400656</td>\n      <td>2.129617</td>\n      <td>0.643447</td>\n      <td>-0.236078</td>\n      <td>0.472530</td>\n      <td>0.201846</td>\n      <td>0.717411</td>\n      <td>0.133039</td>\n      <td>0.387491</td>\n    </tr>\n    <tr>\n      <th>17498</th>\n      <td>-0.875098</td>\n      <td>-0.469244</td>\n      <td>-0.277808</td>\n      <td>-0.281861</td>\n      <td>-0.404335</td>\n      <td>-0.190690</td>\n      <td>-0.472516</td>\n      <td>-0.531565</td>\n      <td>-0.088520</td>\n      <td>-0.282635</td>\n      <td>...</td>\n      <td>0.342480</td>\n      <td>-0.306736</td>\n      <td>-0.397212</td>\n      <td>-0.238033</td>\n      <td>1.597515</td>\n      <td>1.560039</td>\n      <td>0.970806</td>\n      <td>2.351777</td>\n      <td>0.917199</td>\n      <td>-0.685662</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12425</th>\n      <td>-0.857010</td>\n      <td>-0.925512</td>\n      <td>0.251688</td>\n      <td>-0.164870</td>\n      <td>-0.199754</td>\n      <td>0.355163</td>\n      <td>-0.626647</td>\n      <td>0.292643</td>\n      <td>0.462942</td>\n      <td>1.609487</td>\n      <td>...</td>\n      <td>0.527872</td>\n      <td>-0.373559</td>\n      <td>0.006812</td>\n      <td>0.130743</td>\n      <td>-0.344425</td>\n      <td>-0.830342</td>\n      <td>-1.115266</td>\n      <td>-1.616840</td>\n      <td>-0.097982</td>\n      <td>0.934535</td>\n    </tr>\n    <tr>\n      <th>18804</th>\n      <td>0.620468</td>\n      <td>-0.189069</td>\n      <td>-0.213413</td>\n      <td>0.577919</td>\n      <td>-0.091439</td>\n      <td>-0.431539</td>\n      <td>-0.409015</td>\n      <td>0.011714</td>\n      <td>-0.253144</td>\n      <td>-0.052158</td>\n      <td>...</td>\n      <td>-1.190409</td>\n      <td>-1.562879</td>\n      <td>-1.072920</td>\n      <td>-1.005408</td>\n      <td>-1.346681</td>\n      <td>-1.646640</td>\n      <td>-1.393278</td>\n      <td>-1.580012</td>\n      <td>-1.496626</td>\n      <td>-1.511881</td>\n    </tr>\n    <tr>\n      <th>95248</th>\n      <td>-0.139316</td>\n      <td>-0.006462</td>\n      <td>-0.014189</td>\n      <td>0.195926</td>\n      <td>0.074067</td>\n      <td>-0.406737</td>\n      <td>-0.729983</td>\n      <td>-0.191057</td>\n      <td>-0.535689</td>\n      <td>-0.268353</td>\n      <td>...</td>\n      <td>-0.808541</td>\n      <td>0.099055</td>\n      <td>-1.027515</td>\n      <td>-0.075408</td>\n      <td>1.277822</td>\n      <td>-0.745751</td>\n      <td>0.775518</td>\n      <td>-0.400837</td>\n      <td>-0.450499</td>\n      <td>-1.126337</td>\n    </tr>\n    <tr>\n      <th>17587</th>\n      <td>0.383935</td>\n      <td>2.141251</td>\n      <td>0.454010</td>\n      <td>-0.241576</td>\n      <td>-0.262764</td>\n      <td>0.668187</td>\n      <td>0.744732</td>\n      <td>1.478032</td>\n      <td>0.822140</td>\n      <td>-0.563817</td>\n      <td>...</td>\n      <td>-1.697793</td>\n      <td>-0.665030</td>\n      <td>-0.196496</td>\n      <td>-0.881531</td>\n      <td>-0.731217</td>\n      <td>-1.206747</td>\n      <td>-0.750349</td>\n      <td>0.142435</td>\n      <td>-1.031463</td>\n      <td>0.327783</td>\n    </tr>\n    <tr>\n      <th>70936</th>\n      <td>-0.314699</td>\n      <td>-0.537996</td>\n      <td>-0.242504</td>\n      <td>-0.259642</td>\n      <td>-0.648286</td>\n      <td>-0.538052</td>\n      <td>-0.559009</td>\n      <td>-0.371742</td>\n      <td>-0.542160</td>\n      <td>0.329054</td>\n      <td>...</td>\n      <td>0.967511</td>\n      <td>1.325316</td>\n      <td>1.732603</td>\n      <td>0.463518</td>\n      <td>1.145644</td>\n      <td>0.658337</td>\n      <td>1.021091</td>\n      <td>0.453733</td>\n      <td>0.577647</td>\n      <td>0.964795</td>\n    </tr>\n  </tbody>\n</table>\n<p>19922 rows × 84 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = skl.preprocessing.StandardScaler(copy=False)\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# Support vector classification.\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\anaconda3\\envs\\HMAN\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "clf.fit(X_train_np, y_train_np)\n",
    "score = clf.score(X_test_np, y_test_np)\n",
    "print('Accuracy: {:.2%}'.format(score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from kddirkit.networks.encoders import SentenceEncoder\n",
    "from kddirkit.networks.models import BaselineModel\n",
    "from kddirkit.config import *\n",
    "from kddirkit.dataloaders import LoadFMA, LoadHierData\n",
    "from kddirkit.frameworks import Trainer\n",
    "from kddirkit.losses.FocalLoss import FocalLoss\n",
    "\n",
    "parser = Parser(ROOT_PATH + \"/data/config\", \"HMAN\")\n",
    "oneParser = parser.oneParser\n",
    "args, _ = oneParser.parse_known_args(args=[])\n",
    "args = vars(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "HierDataLoader = LoadHierData.HierDataLoader(workdir=os.getcwd(), pattern='default', device=device)\n",
    "genre_levels_Tensor = HierDataLoader.genre_levels_Tensor.to(device)\n",
    "genre_level_layer = HierDataLoader.genre_level_layer\n",
    "\n",
    "trainDataLoader = LoadFMA.FMATrainDataLoader(device=device, feature_mode=\"fma_mfcc\")\n",
    "testDataLoader = LoadFMA.FMATestDataLoader(mode=\"pr\", feature_mode=\"fma_mfcc\", device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#   def __init__(self, input_size, common_size):\n",
    "#     super(MLP, self).__init__()\n",
    "#     self.linear = nn.Sequential(\n",
    "#       nn.Linear(input_size, common_size),\n",
    "#       nn.LogSoftmax(inplace=True)\n",
    "#       # nn.ReLU(inplace=True)\n",
    "#     )\n",
    "#\n",
    "#   def forward(self, x):\n",
    "#     out = self.linear(x)\n",
    "#     return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_units, out_units, dropout=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.linear_1 = nn.Linear(140, 100)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(100, 10)\n",
    "        self.linear_3 = nn.Linear(10, out_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_3(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# fma_data = torch.rand((3, 140)).to(args.device)\n",
    "# test_module = MLP(140, 16).to(args.device)\n",
    "# test_module(fma_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.1610,  4.0913,  1.3561,  ...,  5.5406,  5.8819,  5.3047],\n        [ 2.9958,  0.6376,  0.2148,  ...,  6.6153,  5.6904,  6.6683],\n        [ 3.9623,  5.9557,  0.6369,  ...,  8.0138,  7.3404,  6.9348],\n        ...,\n        [15.3014,  2.5558,  1.1593,  ...,  6.6224,  6.6642,  6.7467],\n        [14.8685,  0.3737, -0.0997,  ...,  8.0130,  8.1268,  7.4578],\n        [-0.1349,  0.9803, -0.0680,  ...,  6.1940,  6.3627,  6.5727]])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "class argparse():\n",
    "    pass\n",
    "args = argparse()\n",
    "args.epochs, args.learning_rate, args.patience = [400, 0.001, 4]\n",
    "# args.hidden_size, args.input_size= [40, 30]\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train_np)\n",
    "X_test_tensor = torch.Tensor(X_test_np)\n",
    "X_val_tensor = torch.Tensor(X_val_np)\n",
    "\n",
    "# y_train_tensor = torch.LongTensor(pd.get_dummies(y_train).to_numpy()).to(args.device)\n",
    "# y_test_tensor = torch.LongTensor(pd.get_dummies(y_test).to_numpy()).to(args.device)\n",
    "# y_val_tensor = torch.LongTensor(pd.get_dummies(y_val).to_numpy()).to(args.device)\n",
    "\n",
    "y_train_tensor = np.zeros((len(y_train_np) , 16))  # 相当于 做了一个onehot_dict\n",
    "y_train_tensor[np.arange(len(y_train_np) ), y_train_np] = 1  # 为onehot_dict 赋值\n",
    "y_train_tensor = torch.LongTensor(y_train_tensor)\n",
    "\n",
    "y_test_tensor = np.zeros((len(y_test_np) , 16))  # 相当于 做了一个onehot_dict\n",
    "y_test_tensor[np.arange(len(y_test_np) ), y_test_np] = 1  # 为onehot_dict 赋值\n",
    "y_test_tensor = torch.LongTensor(y_test_tensor)\n",
    "\n",
    "y_val_tensor = np.zeros((len(y_val_np) , 16))  # 相当于 做了一个onehot_dict\n",
    "y_val_tensor[np.arange(len(y_val_np) ), y_val_np] = 1  # 为onehot_dict 赋值\n",
    "y_val_tensor = torch.LongTensor(y_val_tensor)\n",
    "\n",
    "\n",
    "train_dataset=TensorDataset(X_train_tensor,y_train_tensor)\n",
    "test_dataset=TensorDataset(X_test_tensor,y_test_tensor)\n",
    "val_dataset=TensorDataset(X_val_tensor,y_val_tensor)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=1000 , shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=1000, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(dataset=val_dataset,batch_size=1000, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self,patience=7,verbose=False,delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "    def __call__(self,val_loss,model,path):\n",
    "        print(\"val_loss={}\".format(val_loss))\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss,model,path)\n",
    "        elif score < self.best_score+self.delta:\n",
    "            self.counter+=1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter>=self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss,model,path)\n",
    "            self.counter = 0\n",
    "    def save_checkpoint(self,val_loss,model,path):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path+'/'+'model_checkpoint.pth')\n",
    "        self.val_loss_min = val_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/400,0/20 of train, loss=2.819443941116333, training accuracy = 0.039000000804662704, testing accuracy = 0.05868636071681976\n",
      "epoch=0/400,10/20 of train, loss=2.716132402420044, training accuracy = 0.08190909773111343, testing accuracy = 0.2631169855594635\n",
      "epoch=1/400,0/20 of train, loss=2.5848886966705322, training accuracy = 0.2850000262260437, testing accuracy = 0.2763311266899109\n",
      "epoch=1/400,10/20 of train, loss=2.5885274410247803, training accuracy = 0.28272730112075806, testing accuracy = 0.2763311266899109\n",
      "epoch=2/400,0/20 of train, loss=2.6332662105560303, training accuracy = 0.242000013589859, testing accuracy = 0.2763311266899109\n",
      "epoch=2/400,10/20 of train, loss=2.598578691482544, training accuracy = 0.2805454730987549, testing accuracy = 0.2763311266899109\n",
      "epoch=3/400,0/20 of train, loss=2.614696979522705, training accuracy = 0.26200002431869507, testing accuracy = 0.2763311266899109\n",
      "epoch=3/400,10/20 of train, loss=2.593579053878784, training accuracy = 0.28618183732032776, testing accuracy = 0.2774970829486847\n",
      "epoch=4/400,0/20 of train, loss=2.5680136680603027, training accuracy = 0.3060000240802765, testing accuracy = 0.28371551632881165\n",
      "epoch=4/400,10/20 of train, loss=2.5594382286071777, training accuracy = 0.30154547095298767, testing accuracy = 0.2965410053730011\n",
      "epoch=5/400,0/20 of train, loss=2.6026625633239746, training accuracy = 0.27000001072883606, testing accuracy = 0.29809561371803284\n",
      "epoch=5/400,10/20 of train, loss=2.558928966522217, training accuracy = 0.3068181872367859, testing accuracy = 0.30159348249435425\n",
      "epoch=6/400,0/20 of train, loss=2.5928800106048584, training accuracy = 0.2840000092983246, testing accuracy = 0.30392539501190186\n",
      "epoch=6/400,10/20 of train, loss=2.573981761932373, training accuracy = 0.3097273111343384, testing accuracy = 0.30509135127067566\n",
      "epoch=7/400,0/20 of train, loss=2.577946424484253, training accuracy = 0.30000001192092896, testing accuracy = 0.3066459596157074\n",
      "epoch=7/400,10/20 of train, loss=2.549987554550171, training accuracy = 0.3162727653980255, testing accuracy = 0.3089778423309326\n",
      "epoch=8/400,0/20 of train, loss=2.605480909347534, training accuracy = 0.2680000066757202, testing accuracy = 0.3101437985897064\n",
      "epoch=8/400,10/20 of train, loss=2.5310211181640625, training accuracy = 0.3117273449897766, testing accuracy = 0.31053245067596436\n",
      "epoch=9/400,0/20 of train, loss=2.546985626220703, training accuracy = 0.33400002121925354, testing accuracy = 0.3101437985897064\n",
      "epoch=9/400,10/20 of train, loss=2.5500333309173584, training accuracy = 0.31881818175315857, testing accuracy = 0.3109211027622223\n",
      "epoch=10/400,0/20 of train, loss=2.5754897594451904, training accuracy = 0.30400002002716064, testing accuracy = 0.3085891902446747\n",
      "epoch=10/400,10/20 of train, loss=2.546400547027588, training accuracy = 0.3200909197330475, testing accuracy = 0.3113097548484802\n",
      "epoch=11/400,0/20 of train, loss=2.5647106170654297, training accuracy = 0.30900001525878906, testing accuracy = 0.30936649441719055\n",
      "epoch=11/400,10/20 of train, loss=2.5576891899108887, training accuracy = 0.31881824135780334, testing accuracy = 0.3097551465034485\n",
      "epoch=12/400,0/20 of train, loss=2.5649805068969727, training accuracy = 0.3060000240802765, testing accuracy = 0.3066459596157074\n",
      "epoch=12/400,10/20 of train, loss=2.558318853378296, training accuracy = 0.32172727584838867, testing accuracy = 0.30703458189964294\n",
      "epoch=13/400,0/20 of train, loss=2.557622194290161, training accuracy = 0.3140000104904175, testing accuracy = 0.30820053815841675\n",
      "epoch=13/400,10/20 of train, loss=2.5200865268707275, training accuracy = 0.31836366653442383, testing accuracy = 0.3078118860721588\n",
      "epoch=14/400,0/20 of train, loss=2.5740303993225098, training accuracy = 0.30100002884864807, testing accuracy = 0.30936649441719055\n",
      "epoch=14/400,10/20 of train, loss=2.5454745292663574, training accuracy = 0.32972732186317444, testing accuracy = 0.30703458189964294\n",
      "epoch=15/400,0/20 of train, loss=2.5435447692871094, training accuracy = 0.33000001311302185, testing accuracy = 0.30586865544319153\n",
      "epoch=15/400,10/20 of train, loss=2.543858528137207, training accuracy = 0.32736364006996155, testing accuracy = 0.30275943875312805\n",
      "epoch=16/400,0/20 of train, loss=2.544595241546631, training accuracy = 0.33400002121925354, testing accuracy = 0.30275943875312805\n",
      "epoch=16/400,10/20 of train, loss=2.55502986907959, training accuracy = 0.3284545838832855, testing accuracy = 0.30509135127067566\n",
      "epoch=17/400,0/20 of train, loss=2.554562568664551, training accuracy = 0.32100000977516174, testing accuracy = 0.30625730752944946\n",
      "epoch=17/400,10/20 of train, loss=2.5224719047546387, training accuracy = 0.32836365699768066, testing accuracy = 0.303148090839386\n",
      "epoch=18/400,0/20 of train, loss=2.5612926483154297, training accuracy = 0.3110000193119049, testing accuracy = 0.3074232339859009\n",
      "epoch=18/400,10/20 of train, loss=2.5596535205841064, training accuracy = 0.3258182108402252, testing accuracy = 0.30625730752944946\n",
      "epoch=19/400,0/20 of train, loss=2.5661144256591797, training accuracy = 0.30400002002716064, testing accuracy = 0.30703458189964294\n",
      "epoch=19/400,10/20 of train, loss=2.5818750858306885, training accuracy = 0.31854549050331116, testing accuracy = 0.3043140470981598\n",
      "epoch=20/400,0/20 of train, loss=2.5549209117889404, training accuracy = 0.31700000166893005, testing accuracy = 0.30703458189964294\n",
      "epoch=20/400,10/20 of train, loss=2.551180601119995, training accuracy = 0.32100003957748413, testing accuracy = 0.29926156997680664\n",
      "epoch=21/400,0/20 of train, loss=2.5615429878234863, training accuracy = 0.31800001859664917, testing accuracy = 0.30042752623558044\n",
      "epoch=21/400,10/20 of train, loss=2.5397090911865234, training accuracy = 0.32918184995651245, testing accuracy = 0.29926156997680664\n",
      "epoch=22/400,0/20 of train, loss=2.5806329250335693, training accuracy = 0.29100000858306885, testing accuracy = 0.3000388741493225\n",
      "epoch=22/400,10/20 of train, loss=2.544823408126831, training accuracy = 0.32254549860954285, testing accuracy = 0.30625730752944946\n",
      "epoch=23/400,0/20 of train, loss=2.559285879135132, training accuracy = 0.31300002336502075, testing accuracy = 0.3054800033569336\n",
      "epoch=23/400,10/20 of train, loss=2.542062282562256, training accuracy = 0.32445457577705383, testing accuracy = 0.30703458189964294\n",
      "epoch=24/400,0/20 of train, loss=2.519275426864624, training accuracy = 0.3570000231266022, testing accuracy = 0.30392539501190186\n",
      "epoch=24/400,10/20 of train, loss=2.5907979011535645, training accuracy = 0.3237272799015045, testing accuracy = 0.3000388741493225\n",
      "epoch=25/400,0/20 of train, loss=2.555574893951416, training accuracy = 0.31800001859664917, testing accuracy = 0.3008161783218384\n",
      "epoch=25/400,10/20 of train, loss=2.5237972736358643, training accuracy = 0.32254546880722046, testing accuracy = 0.30703458189964294\n",
      "epoch=26/400,0/20 of train, loss=2.5425076484680176, training accuracy = 0.3330000042915344, testing accuracy = 0.3078118860721588\n",
      "epoch=26/400,10/20 of train, loss=2.5378425121307373, training accuracy = 0.3267272710800171, testing accuracy = 0.3066459596157074\n",
      "epoch=27/400,0/20 of train, loss=2.5783767700195312, training accuracy = 0.2930000126361847, testing accuracy = 0.3078118860721588\n",
      "epoch=27/400,10/20 of train, loss=2.5308284759521484, training accuracy = 0.3264545500278473, testing accuracy = 0.3085891902446747\n",
      "epoch=28/400,0/20 of train, loss=2.5679917335510254, training accuracy = 0.30300000309944153, testing accuracy = 0.3089778423309326\n",
      "epoch=28/400,10/20 of train, loss=2.5380606651306152, training accuracy = 0.3248182237148285, testing accuracy = 0.30936649441719055\n",
      "epoch=29/400,0/20 of train, loss=2.5428595542907715, training accuracy = 0.3320000171661377, testing accuracy = 0.30936649441719055\n",
      "epoch=29/400,10/20 of train, loss=2.5472724437713623, training accuracy = 0.3322727680206299, testing accuracy = 0.30936649441719055\n",
      "epoch=30/400,0/20 of train, loss=2.5534162521362305, training accuracy = 0.32100000977516174, testing accuracy = 0.3085891902446747\n",
      "epoch=30/400,10/20 of train, loss=2.5251636505126953, training accuracy = 0.3270000219345093, testing accuracy = 0.3078118860721588\n",
      "epoch=31/400,0/20 of train, loss=2.560211658477783, training accuracy = 0.3150000274181366, testing accuracy = 0.3074232339859009\n",
      "epoch=31/400,10/20 of train, loss=2.5640499591827393, training accuracy = 0.3264545500278473, testing accuracy = 0.30703458189964294\n",
      "epoch=32/400,0/20 of train, loss=2.5559418201446533, training accuracy = 0.3160000145435333, testing accuracy = 0.3085891902446747\n",
      "epoch=32/400,10/20 of train, loss=2.541917562484741, training accuracy = 0.32500001788139343, testing accuracy = 0.30936649441719055\n",
      "epoch=33/400,0/20 of train, loss=2.5372726917266846, training accuracy = 0.3410000205039978, testing accuracy = 0.30936649441719055\n",
      "epoch=33/400,10/20 of train, loss=2.5480644702911377, training accuracy = 0.3307272791862488, testing accuracy = 0.30820053815841675\n",
      "epoch=34/400,0/20 of train, loss=2.5557589530944824, training accuracy = 0.320000022649765, testing accuracy = 0.3078118860721588\n",
      "epoch=34/400,10/20 of train, loss=2.5264108180999756, training accuracy = 0.32736366987228394, testing accuracy = 0.3120870590209961\n",
      "epoch=35/400,0/20 of train, loss=2.5662598609924316, training accuracy = 0.30400002002716064, testing accuracy = 0.30936649441719055\n",
      "epoch=35/400,10/20 of train, loss=2.5444042682647705, training accuracy = 0.3271818459033966, testing accuracy = 0.3089778423309326\n",
      "epoch=36/400,0/20 of train, loss=2.5565648078918457, training accuracy = 0.3190000057220459, testing accuracy = 0.3085891902446747\n",
      "epoch=36/400,10/20 of train, loss=2.564784049987793, training accuracy = 0.33127275109291077, testing accuracy = 0.3089778423309326\n",
      "epoch=37/400,0/20 of train, loss=2.5496957302093506, training accuracy = 0.32600000500679016, testing accuracy = 0.3085891902446747\n",
      "epoch=37/400,10/20 of train, loss=2.530687093734741, training accuracy = 0.32972732186317444, testing accuracy = 0.3097551465034485\n",
      "epoch=38/400,0/20 of train, loss=2.5295917987823486, training accuracy = 0.33900001645088196, testing accuracy = 0.3109211027622223\n",
      "epoch=38/400,10/20 of train, loss=2.527432680130005, training accuracy = 0.3327272832393646, testing accuracy = 0.3097551465034485\n",
      "epoch=39/400,0/20 of train, loss=2.537059783935547, training accuracy = 0.34300002455711365, testing accuracy = 0.3120870590209961\n",
      "epoch=39/400,10/20 of train, loss=2.538618803024292, training accuracy = 0.3272727429866791, testing accuracy = 0.3097551465034485\n",
      "epoch=40/400,0/20 of train, loss=2.5296895503997803, training accuracy = 0.3440000116825104, testing accuracy = 0.3085891902446747\n",
      "epoch=40/400,10/20 of train, loss=2.561630964279175, training accuracy = 0.3338181972503662, testing accuracy = 0.3109211027622223\n",
      "epoch=41/400,0/20 of train, loss=2.562135934829712, training accuracy = 0.31300002336502075, testing accuracy = 0.3113097548484802\n",
      "epoch=41/400,10/20 of train, loss=2.527226209640503, training accuracy = 0.33236366510391235, testing accuracy = 0.3085891902446747\n",
      "epoch=42/400,0/20 of train, loss=2.5498409271240234, training accuracy = 0.3190000057220459, testing accuracy = 0.31053245067596436\n",
      "epoch=42/400,10/20 of train, loss=2.542701005935669, training accuracy = 0.3299091160297394, testing accuracy = 0.3101437985897064\n",
      "epoch=43/400,0/20 of train, loss=2.564514398574829, training accuracy = 0.3070000112056732, testing accuracy = 0.30820053815841675\n",
      "epoch=43/400,10/20 of train, loss=2.5485599040985107, training accuracy = 0.3277273178100586, testing accuracy = 0.30703458189964294\n",
      "epoch=44/400,0/20 of train, loss=2.5420162677764893, training accuracy = 0.3360000252723694, testing accuracy = 0.3043140470981598\n",
      "epoch=44/400,10/20 of train, loss=2.529096841812134, training accuracy = 0.33190909028053284, testing accuracy = 0.3054800033569336\n",
      "epoch=45/400,0/20 of train, loss=2.5745363235473633, training accuracy = 0.29100000858306885, testing accuracy = 0.30936649441719055\n",
      "epoch=45/400,10/20 of train, loss=2.5399277210235596, training accuracy = 0.3298182487487793, testing accuracy = 0.3120870590209961\n",
      "epoch=46/400,0/20 of train, loss=2.5508599281311035, training accuracy = 0.32200002670288086, testing accuracy = 0.30936649441719055\n",
      "epoch=46/400,10/20 of train, loss=2.5511257648468018, training accuracy = 0.33336368203163147, testing accuracy = 0.31053245067596436\n",
      "epoch=47/400,0/20 of train, loss=2.560540199279785, training accuracy = 0.31200000643730164, testing accuracy = 0.31053245067596436\n",
      "epoch=47/400,10/20 of train, loss=2.5205647945404053, training accuracy = 0.33263638615608215, testing accuracy = 0.30936649441719055\n",
      "epoch=48/400,0/20 of train, loss=2.5508921146392822, training accuracy = 0.32500001788139343, testing accuracy = 0.30586865544319153\n",
      "epoch=48/400,10/20 of train, loss=2.5602192878723145, training accuracy = 0.332181841135025, testing accuracy = 0.3097551465034485\n",
      "epoch=49/400,0/20 of train, loss=2.545212984085083, training accuracy = 0.3270000219345093, testing accuracy = 0.31403031945228577\n",
      "epoch=49/400,10/20 of train, loss=2.5401909351348877, training accuracy = 0.33472728729248047, testing accuracy = 0.31053245067596436\n",
      "epoch=50/400,0/20 of train, loss=2.5472145080566406, training accuracy = 0.3230000138282776, testing accuracy = 0.30936649441719055\n",
      "epoch=50/400,10/20 of train, loss=2.523966073989868, training accuracy = 0.3330000340938568, testing accuracy = 0.3120870590209961\n",
      "epoch=51/400,0/20 of train, loss=2.5549609661102295, training accuracy = 0.32100000977516174, testing accuracy = 0.3097551465034485\n",
      "epoch=51/400,10/20 of train, loss=2.539527177810669, training accuracy = 0.33209091424942017, testing accuracy = 0.31169840693473816\n",
      "epoch=52/400,0/20 of train, loss=2.533120632171631, training accuracy = 0.34200000762939453, testing accuracy = 0.3089778423309326\n",
      "epoch=52/400,10/20 of train, loss=2.521491765975952, training accuracy = 0.33500000834465027, testing accuracy = 0.3089778423309326\n",
      "epoch=53/400,0/20 of train, loss=2.558500051498413, training accuracy = 0.31800001859664917, testing accuracy = 0.3120870590209961\n",
      "epoch=53/400,10/20 of train, loss=2.5443379878997803, training accuracy = 0.3322727680206299, testing accuracy = 0.3089778423309326\n",
      "epoch=54/400,0/20 of train, loss=2.5438194274902344, training accuracy = 0.328000009059906, testing accuracy = 0.3101437985897064\n",
      "epoch=54/400,10/20 of train, loss=2.5290470123291016, training accuracy = 0.33290910720825195, testing accuracy = 0.31053245067596436\n",
      "epoch=55/400,0/20 of train, loss=2.53448748588562, training accuracy = 0.3370000123977661, testing accuracy = 0.3120870590209961\n",
      "epoch=55/400,10/20 of train, loss=2.518920660018921, training accuracy = 0.3356363773345947, testing accuracy = 0.3085891902446747\n",
      "epoch=56/400,0/20 of train, loss=2.534085273742676, training accuracy = 0.3290000259876251, testing accuracy = 0.3575592637062073\n",
      "epoch=56/400,10/20 of train, loss=2.5093164443969727, training accuracy = 0.36763638257980347, testing accuracy = 0.3548387289047241\n",
      "epoch=57/400,0/20 of train, loss=2.4938392639160156, training accuracy = 0.37800002098083496, testing accuracy = 0.35289546847343445\n",
      "epoch=57/400,10/20 of train, loss=2.4946775436401367, training accuracy = 0.3762727677822113, testing accuracy = 0.36610960960388184\n",
      "epoch=58/400,0/20 of train, loss=2.525338888168335, training accuracy = 0.35100001096725464, testing accuracy = 0.36300039291381836\n",
      "epoch=58/400,10/20 of train, loss=2.510317802429199, training accuracy = 0.3803636431694031, testing accuracy = 0.36610960960388184\n",
      "epoch=59/400,0/20 of train, loss=2.4938507080078125, training accuracy = 0.38200002908706665, testing accuracy = 0.3680528700351715\n",
      "epoch=59/400,10/20 of train, loss=2.5041027069091797, training accuracy = 0.37727275490760803, testing accuracy = 0.3692188262939453\n",
      "epoch=60/400,0/20 of train, loss=2.5242881774902344, training accuracy = 0.35200002789497375, testing accuracy = 0.36377769708633423\n",
      "epoch=60/400,10/20 of train, loss=2.4960994720458984, training accuracy = 0.3767273426055908, testing accuracy = 0.3626117408275604\n",
      "epoch=61/400,0/20 of train, loss=2.5265259742736816, training accuracy = 0.34700000286102295, testing accuracy = 0.3676642179489136\n",
      "epoch=61/400,10/20 of train, loss=2.4080631732940674, training accuracy = 0.4007272720336914, testing accuracy = 0.448503702878952\n",
      "epoch=62/400,0/20 of train, loss=2.4256539344787598, training accuracy = 0.4520000219345093, testing accuracy = 0.45666536688804626\n",
      "epoch=62/400,10/20 of train, loss=2.411008834838867, training accuracy = 0.4708182215690613, testing accuracy = 0.4694908857345581\n",
      "epoch=63/400,0/20 of train, loss=2.3921592235565186, training accuracy = 0.4820000231266022, testing accuracy = 0.4780412018299103\n",
      "epoch=63/400,10/20 of train, loss=2.360583782196045, training accuracy = 0.48890918493270874, testing accuracy = 0.473766028881073\n",
      "epoch=64/400,0/20 of train, loss=2.4117066860198975, training accuracy = 0.4610000252723694, testing accuracy = 0.4830936789512634\n",
      "epoch=64/400,10/20 of train, loss=2.3677492141723633, training accuracy = 0.5111818313598633, testing accuracy = 0.4959191679954529\n",
      "epoch=65/400,0/20 of train, loss=2.3666036128997803, training accuracy = 0.51500004529953, testing accuracy = 0.4939759075641632\n",
      "epoch=65/400,10/20 of train, loss=2.371805191040039, training accuracy = 0.5190001130104065, testing accuracy = 0.5083560347557068\n",
      "epoch=66/400,0/20 of train, loss=2.3780205249786377, training accuracy = 0.49500003457069397, testing accuracy = 0.5044695138931274\n",
      "epoch=66/400,10/20 of train, loss=2.351680278778076, training accuracy = 0.5263636708259583, testing accuracy = 0.5204042196273804\n",
      "epoch=67/400,0/20 of train, loss=2.4025092124938965, training accuracy = 0.4750000238418579, testing accuracy = 0.5157403945922852\n",
      "epoch=67/400,10/20 of train, loss=2.3173365592956543, training accuracy = 0.5381818413734436, testing accuracy = 0.5273999571800232\n",
      "epoch=68/400,0/20 of train, loss=2.3884494304656982, training accuracy = 0.48600003123283386, testing accuracy = 0.5176836252212524\n",
      "epoch=68/400,10/20 of train, loss=2.3202855587005615, training accuracy = 0.5425454378128052, testing accuracy = 0.5246793627738953\n",
      "epoch=69/400,0/20 of train, loss=2.365257501602173, training accuracy = 0.5070000290870667, testing accuracy = 0.5258453488349915\n",
      "epoch=69/400,10/20 of train, loss=2.3279688358306885, training accuracy = 0.5456364154815674, testing accuracy = 0.5266226530075073\n",
      "epoch=70/400,0/20 of train, loss=2.423365354537964, training accuracy = 0.453000009059906, testing accuracy = 0.5285658836364746\n",
      "epoch=70/400,10/20 of train, loss=2.3329555988311768, training accuracy = 0.5437272787094116, testing accuracy = 0.526233971118927\n",
      "epoch=71/400,0/20 of train, loss=2.38494873046875, training accuracy = 0.48900002241134644, testing accuracy = 0.5305091738700867\n",
      "epoch=71/400,10/20 of train, loss=2.317971706390381, training accuracy = 0.5496364235877991, testing accuracy = 0.5211815237998962\n",
      "epoch=72/400,0/20 of train, loss=2.3460161685943604, training accuracy = 0.5300000309944153, testing accuracy = 0.5308977961540222\n",
      "epoch=72/400,10/20 of train, loss=2.3277711868286133, training accuracy = 0.5465455651283264, testing accuracy = 0.5289545655250549\n",
      "epoch=73/400,0/20 of train, loss=2.3697118759155273, training accuracy = 0.5100000500679016, testing accuracy = 0.5134084820747375\n",
      "epoch=73/400,10/20 of train, loss=2.2995805740356445, training accuracy = 0.5506364107131958, testing accuracy = 0.5258453488349915\n",
      "epoch=74/400,0/20 of train, loss=2.373427391052246, training accuracy = 0.5060000419616699, testing accuracy = 0.5297318696975708\n",
      "epoch=74/400,10/20 of train, loss=2.3539562225341797, training accuracy = 0.5536364912986755, testing accuracy = 0.5308977961540222\n",
      "epoch=75/400,0/20 of train, loss=2.368528366088867, training accuracy = 0.5040000081062317, testing accuracy = 0.5270112752914429\n",
      "epoch=75/400,10/20 of train, loss=2.3509514331817627, training accuracy = 0.554545521736145, testing accuracy = 0.5215701460838318\n",
      "epoch=76/400,0/20 of train, loss=2.3664815425872803, training accuracy = 0.5070000290870667, testing accuracy = 0.5231247544288635\n",
      "epoch=76/400,10/20 of train, loss=2.3004956245422363, training accuracy = 0.5561819076538086, testing accuracy = 0.516517698764801\n",
      "epoch=77/400,0/20 of train, loss=2.3853893280029297, training accuracy = 0.48000001907348633, testing accuracy = 0.5211815237998962\n",
      "epoch=77/400,10/20 of train, loss=2.326002836227417, training accuracy = 0.548090934753418, testing accuracy = 0.5270112752914429\n",
      "epoch=78/400,0/20 of train, loss=2.3765616416931152, training accuracy = 0.503000020980835, testing accuracy = 0.5293431878089905\n",
      "epoch=78/400,10/20 of train, loss=2.3166310787200928, training accuracy = 0.5567272901535034, testing accuracy = 0.522736132144928\n",
      "epoch=79/400,0/20 of train, loss=2.34649920463562, training accuracy = 0.5260000228881836, testing accuracy = 0.5285658836364746\n",
      "epoch=79/400,10/20 of train, loss=2.319711446762085, training accuracy = 0.5618182420730591, testing accuracy = 0.5266226530075073\n",
      "epoch=80/400,0/20 of train, loss=2.3773937225341797, training accuracy = 0.49800002574920654, testing accuracy = 0.5273999571800232\n",
      "epoch=80/400,10/20 of train, loss=2.3150267601013184, training accuracy = 0.5470000505447388, testing accuracy = 0.5351729393005371\n",
      "epoch=81/400,0/20 of train, loss=2.35856294631958, training accuracy = 0.5160000324249268, testing accuracy = 0.526233971118927\n",
      "epoch=81/400,10/20 of train, loss=2.295438051223755, training accuracy = 0.563090980052948, testing accuracy = 0.5270112752914429\n",
      "epoch=82/400,0/20 of train, loss=2.3316802978515625, training accuracy = 0.5470000505447388, testing accuracy = 0.5273999571800232\n",
      "epoch=82/400,10/20 of train, loss=2.329967975616455, training accuracy = 0.5634546279907227, testing accuracy = 0.5196269154548645\n",
      "epoch=83/400,0/20 of train, loss=2.3664166927337646, training accuracy = 0.5080000162124634, testing accuracy = 0.5363389253616333\n",
      "epoch=83/400,10/20 of train, loss=2.3365023136138916, training accuracy = 0.558181881904602, testing accuracy = 0.5336183309555054\n",
      "epoch=84/400,0/20 of train, loss=2.381328821182251, training accuracy = 0.48900002241134644, testing accuracy = 0.5184609293937683\n",
      "epoch=84/400,10/20 of train, loss=2.3023459911346436, training accuracy = 0.5624545812606812, testing accuracy = 0.5289545655250549\n",
      "epoch=85/400,0/20 of train, loss=2.3682785034179688, training accuracy = 0.5020000338554382, testing accuracy = 0.5347843170166016\n",
      "epoch=85/400,10/20 of train, loss=2.3075969219207764, training accuracy = 0.5681818723678589, testing accuracy = 0.5351729393005371\n",
      "epoch=86/400,0/20 of train, loss=2.3851242065429688, training accuracy = 0.4870000183582306, testing accuracy = 0.5270112752914429\n",
      "epoch=86/400,10/20 of train, loss=2.314375162124634, training accuracy = 0.5590909719467163, testing accuracy = 0.5196269154548645\n",
      "epoch=87/400,0/20 of train, loss=2.3760011196136475, training accuracy = 0.4960000216960907, testing accuracy = 0.5242907404899597\n",
      "epoch=87/400,10/20 of train, loss=2.30924129486084, training accuracy = 0.5616363883018494, testing accuracy = 0.5308977961540222\n",
      "epoch=88/400,0/20 of train, loss=2.361616611480713, training accuracy = 0.5110000371932983, testing accuracy = 0.5266226530075073\n",
      "epoch=88/400,10/20 of train, loss=2.312932014465332, training accuracy = 0.5730909109115601, testing accuracy = 0.5320637226104736\n",
      "epoch=89/400,0/20 of train, loss=2.387744188308716, training accuracy = 0.484000027179718, testing accuracy = 0.5231247544288635\n",
      "epoch=89/400,10/20 of train, loss=2.2660675048828125, training accuracy = 0.568000078201294, testing accuracy = 0.5270112752914429\n",
      "epoch=90/400,0/20 of train, loss=2.3645646572113037, training accuracy = 0.5139999985694885, testing accuracy = 0.5289545655250549\n",
      "epoch=90/400,10/20 of train, loss=2.301910638809204, training accuracy = 0.571363627910614, testing accuracy = 0.5273999571800232\n",
      "epoch=91/400,0/20 of train, loss=2.3660547733306885, training accuracy = 0.5080000162124634, testing accuracy = 0.5254566669464111\n",
      "epoch=91/400,10/20 of train, loss=2.2908761501312256, training accuracy = 0.5645455718040466, testing accuracy = 0.5289545655250549\n",
      "epoch=92/400,0/20 of train, loss=2.359081983566284, training accuracy = 0.51500004529953, testing accuracy = 0.5301204919815063\n",
      "epoch=92/400,10/20 of train, loss=2.31644344329834, training accuracy = 0.5631818771362305, testing accuracy = 0.5246793627738953\n",
      "epoch=93/400,0/20 of train, loss=2.3579118251800537, training accuracy = 0.5170000195503235, testing accuracy = 0.5297318696975708\n",
      "epoch=93/400,10/20 of train, loss=2.2943546772003174, training accuracy = 0.5698181986808777, testing accuracy = 0.5289545655250549\n",
      "epoch=94/400,0/20 of train, loss=2.3427557945251465, training accuracy = 0.5350000262260437, testing accuracy = 0.5301204919815063\n",
      "epoch=94/400,10/20 of train, loss=2.3020172119140625, training accuracy = 0.5778182744979858, testing accuracy = 0.5281772613525391\n",
      "epoch=95/400,0/20 of train, loss=2.3731868267059326, training accuracy = 0.5020000338554382, testing accuracy = 0.5312864184379578\n",
      "epoch=95/400,10/20 of train, loss=2.2930474281311035, training accuracy = 0.5710909962654114, testing accuracy = 0.5351729393005371\n",
      "epoch=96/400,0/20 of train, loss=2.3310983180999756, training accuracy = 0.5430000424385071, testing accuracy = 0.5281772613525391\n",
      "epoch=96/400,10/20 of train, loss=2.2939600944519043, training accuracy = 0.5738182067871094, testing accuracy = 0.5363389253616333\n",
      "epoch=97/400,0/20 of train, loss=2.3675997257232666, training accuracy = 0.5049999952316284, testing accuracy = 0.5293431878089905\n",
      "epoch=97/400,10/20 of train, loss=2.2973134517669678, training accuracy = 0.5730909109115601, testing accuracy = 0.5316751003265381\n",
      "epoch=98/400,0/20 of train, loss=2.350668430328369, training accuracy = 0.527999997138977, testing accuracy = 0.5336183309555054\n",
      "epoch=98/400,10/20 of train, loss=2.2726693153381348, training accuracy = 0.5803636908531189, testing accuracy = 0.532452404499054\n",
      "epoch=99/400,0/20 of train, loss=2.3702824115753174, training accuracy = 0.5010000467300415, testing accuracy = 0.5281772613525391\n",
      "epoch=99/400,10/20 of train, loss=2.2550528049468994, training accuracy = 0.5793636441230774, testing accuracy = 0.5258453488349915\n",
      "epoch=100/400,0/20 of train, loss=2.359600782394409, training accuracy = 0.5170000195503235, testing accuracy = 0.5250680446624756\n",
      "epoch=100/400,10/20 of train, loss=2.2880449295043945, training accuracy = 0.5751818418502808, testing accuracy = 0.5301204919815063\n",
      "epoch=101/400,0/20 of train, loss=2.3387744426727295, training accuracy = 0.534000039100647, testing accuracy = 0.5297318696975708\n",
      "epoch=101/400,10/20 of train, loss=2.274883508682251, training accuracy = 0.5800909996032715, testing accuracy = 0.5316751003265381\n",
      "epoch=102/400,0/20 of train, loss=2.3461899757385254, training accuracy = 0.5270000100135803, testing accuracy = 0.5343956351280212\n",
      "epoch=102/400,10/20 of train, loss=2.305999517440796, training accuracy = 0.5788182616233826, testing accuracy = 0.532452404499054\n",
      "epoch=103/400,0/20 of train, loss=2.3625006675720215, training accuracy = 0.5139999985694885, testing accuracy = 0.5336183309555054\n",
      "epoch=103/400,10/20 of train, loss=2.2894608974456787, training accuracy = 0.5812727212905884, testing accuracy = 0.5301204919815063\n",
      "epoch=104/400,0/20 of train, loss=2.356001138687134, training accuracy = 0.5200000405311584, testing accuracy = 0.5305091738700867\n",
      "epoch=104/400,10/20 of train, loss=2.3041694164276123, training accuracy = 0.5738182067871094, testing accuracy = 0.5305091738700867\n",
      "epoch=105/400,0/20 of train, loss=2.364504098892212, training accuracy = 0.5110000371932983, testing accuracy = 0.5312864184379578\n",
      "epoch=105/400,10/20 of train, loss=2.325143575668335, training accuracy = 0.5790910124778748, testing accuracy = 0.5336183309555054\n",
      "epoch=106/400,0/20 of train, loss=2.3427999019622803, training accuracy = 0.5270000100135803, testing accuracy = 0.5289545655250549\n",
      "epoch=106/400,10/20 of train, loss=2.289259195327759, training accuracy = 0.5736364126205444, testing accuracy = 0.5332297086715698\n",
      "epoch=107/400,0/20 of train, loss=2.344379425048828, training accuracy = 0.531000018119812, testing accuracy = 0.5266226530075073\n",
      "epoch=107/400,10/20 of train, loss=2.2860772609710693, training accuracy = 0.5812727808952332, testing accuracy = 0.5281772613525391\n",
      "epoch=108/400,0/20 of train, loss=2.3638956546783447, training accuracy = 0.5049999952316284, testing accuracy = 0.5328410267829895\n",
      "epoch=108/400,10/20 of train, loss=2.2836709022521973, training accuracy = 0.5800000429153442, testing accuracy = 0.5277885794639587\n",
      "epoch=109/400,0/20 of train, loss=2.3732588291168213, training accuracy = 0.503000020980835, testing accuracy = 0.5215701460838318\n",
      "epoch=109/400,10/20 of train, loss=2.2839443683624268, training accuracy = 0.5780909657478333, testing accuracy = 0.5289545655250549\n",
      "epoch=110/400,0/20 of train, loss=2.357544183731079, training accuracy = 0.5120000243186951, testing accuracy = 0.5308977961540222\n",
      "epoch=110/400,10/20 of train, loss=2.2800629138946533, training accuracy = 0.5828182101249695, testing accuracy = 0.5250680446624756\n",
      "epoch=111/400,0/20 of train, loss=2.3324084281921387, training accuracy = 0.5460000038146973, testing accuracy = 0.5250680446624756\n",
      "epoch=111/400,10/20 of train, loss=2.2863614559173584, training accuracy = 0.5861818194389343, testing accuracy = 0.5328410267829895\n",
      "epoch=112/400,0/20 of train, loss=2.3311705589294434, training accuracy = 0.5410000085830688, testing accuracy = 0.5285658836364746\n",
      "epoch=112/400,10/20 of train, loss=2.2918593883514404, training accuracy = 0.5823637247085571, testing accuracy = 0.5297318696975708\n",
      "epoch=113/400,0/20 of train, loss=2.3509581089019775, training accuracy = 0.5250000357627869, testing accuracy = 0.5351729393005371\n",
      "epoch=113/400,10/20 of train, loss=2.3016977310180664, training accuracy = 0.578272819519043, testing accuracy = 0.5332297086715698\n",
      "epoch=114/400,0/20 of train, loss=2.366971731185913, training accuracy = 0.5070000290870667, testing accuracy = 0.5305091738700867\n",
      "epoch=114/400,10/20 of train, loss=2.2663991451263428, training accuracy = 0.5820909142494202, testing accuracy = 0.5382821559906006\n",
      "epoch=115/400,0/20 of train, loss=2.353130340576172, training accuracy = 0.5200000405311584, testing accuracy = 0.5316751003265381\n",
      "epoch=115/400,10/20 of train, loss=2.302802324295044, training accuracy = 0.5840000510215759, testing accuracy = 0.5332297086715698\n",
      "epoch=116/400,0/20 of train, loss=2.342503070831299, training accuracy = 0.5290000438690186, testing accuracy = 0.5316751003265381\n",
      "epoch=116/400,10/20 of train, loss=2.2854301929473877, training accuracy = 0.5839091539382935, testing accuracy = 0.5316751003265381\n",
      "epoch=117/400,0/20 of train, loss=2.3336739540100098, training accuracy = 0.5410000085830688, testing accuracy = 0.5328410267829895\n",
      "epoch=117/400,10/20 of train, loss=2.2786500453948975, training accuracy = 0.5858181715011597, testing accuracy = 0.5305091738700867\n",
      "epoch=118/400,0/20 of train, loss=2.3771491050720215, training accuracy = 0.4930000305175781, testing accuracy = 0.5340070128440857\n",
      "epoch=118/400,10/20 of train, loss=2.26769757270813, training accuracy = 0.580181896686554, testing accuracy = 0.5332297086715698\n",
      "epoch=119/400,0/20 of train, loss=2.3658573627471924, training accuracy = 0.5049999952316284, testing accuracy = 0.5308977961540222\n",
      "epoch=119/400,10/20 of train, loss=2.276458978652954, training accuracy = 0.5887272953987122, testing accuracy = 0.5305091738700867\n",
      "epoch=120/400,0/20 of train, loss=2.3578288555145264, training accuracy = 0.5170000195503235, testing accuracy = 0.5281772613525391\n",
      "epoch=120/400,10/20 of train, loss=2.2731080055236816, training accuracy = 0.5832727551460266, testing accuracy = 0.5316751003265381\n",
      "epoch=121/400,0/20 of train, loss=2.340024948120117, training accuracy = 0.5360000133514404, testing accuracy = 0.5328410267829895\n",
      "epoch=121/400,10/20 of train, loss=2.277488946914673, training accuracy = 0.5887272953987122, testing accuracy = 0.5289545655250549\n",
      "epoch=122/400,0/20 of train, loss=2.3789122104644775, training accuracy = 0.4930000305175781, testing accuracy = 0.5293431878089905\n",
      "epoch=122/400,10/20 of train, loss=2.2667417526245117, training accuracy = 0.5839091539382935, testing accuracy = 0.532452404499054\n",
      "epoch=123/400,0/20 of train, loss=2.3223750591278076, training accuracy = 0.5520000457763672, testing accuracy = 0.5250680446624756\n",
      "epoch=123/400,10/20 of train, loss=2.3060455322265625, training accuracy = 0.5825454592704773, testing accuracy = 0.5273999571800232\n",
      "epoch=124/400,0/20 of train, loss=2.3480734825134277, training accuracy = 0.5260000228881836, testing accuracy = 0.5343956351280212\n",
      "epoch=124/400,10/20 of train, loss=2.274174213409424, training accuracy = 0.5864546895027161, testing accuracy = 0.5308977961540222\n",
      "epoch=125/400,0/20 of train, loss=2.320997714996338, training accuracy = 0.5530000329017639, testing accuracy = 0.5363389253616333\n",
      "epoch=125/400,10/20 of train, loss=2.2942447662353516, training accuracy = 0.5850909352302551, testing accuracy = 0.5332297086715698\n",
      "epoch=126/400,0/20 of train, loss=2.341210126876831, training accuracy = 0.531000018119812, testing accuracy = 0.5312864184379578\n",
      "epoch=126/400,10/20 of train, loss=2.2878623008728027, training accuracy = 0.5858182311058044, testing accuracy = 0.5312864184379578\n",
      "epoch=127/400,0/20 of train, loss=2.364455223083496, training accuracy = 0.5060000419616699, testing accuracy = 0.5336183309555054\n",
      "epoch=127/400,10/20 of train, loss=2.281672477722168, training accuracy = 0.5851818323135376, testing accuracy = 0.5297318696975708\n",
      "epoch=128/400,0/20 of train, loss=2.3730673789978027, training accuracy = 0.4970000088214874, testing accuracy = 0.5301204919815063\n",
      "epoch=128/400,10/20 of train, loss=2.2912352085113525, training accuracy = 0.5833637118339539, testing accuracy = 0.5270112752914429\n",
      "epoch=129/400,0/20 of train, loss=2.3507959842681885, training accuracy = 0.5210000276565552, testing accuracy = 0.526233971118927\n",
      "epoch=129/400,10/20 of train, loss=2.2717702388763428, training accuracy = 0.5870909690856934, testing accuracy = 0.5316751003265381\n",
      "epoch=130/400,0/20 of train, loss=2.308509111404419, training accuracy = 0.5660000443458557, testing accuracy = 0.5281772613525391\n",
      "epoch=130/400,10/20 of train, loss=2.2838711738586426, training accuracy = 0.5874544978141785, testing accuracy = 0.5371162295341492\n",
      "epoch=131/400,0/20 of train, loss=2.3540639877319336, training accuracy = 0.5190000534057617, testing accuracy = 0.5343956351280212\n",
      "epoch=131/400,10/20 of train, loss=2.271991729736328, training accuracy = 0.5929091572761536, testing accuracy = 0.5277885794639587\n",
      "epoch=132/400,0/20 of train, loss=2.329505681991577, training accuracy = 0.5460000038146973, testing accuracy = 0.526233971118927\n",
      "epoch=132/400,10/20 of train, loss=2.2756714820861816, training accuracy = 0.5925455689430237, testing accuracy = 0.5367275476455688\n",
      "epoch=133/400,0/20 of train, loss=2.3476030826568604, training accuracy = 0.5300000309944153, testing accuracy = 0.5316751003265381\n",
      "epoch=133/400,10/20 of train, loss=2.3187272548675537, training accuracy = 0.5909091234207153, testing accuracy = 0.5347843170166016\n",
      "epoch=134/400,0/20 of train, loss=2.344050407409668, training accuracy = 0.531000018119812, testing accuracy = 0.5363389253616333\n",
      "epoch=134/400,10/20 of train, loss=2.312943935394287, training accuracy = 0.5815454721450806, testing accuracy = 0.5328410267829895\n",
      "epoch=135/400,0/20 of train, loss=2.337672472000122, training accuracy = 0.5350000262260437, testing accuracy = 0.5277885794639587\n",
      "epoch=135/400,10/20 of train, loss=2.2839419841766357, training accuracy = 0.5943636894226074, testing accuracy = 0.5297318696975708\n",
      "epoch=136/400,0/20 of train, loss=2.361603021621704, training accuracy = 0.5080000162124634, testing accuracy = 0.537893533706665\n",
      "epoch=136/400,10/20 of train, loss=2.274033546447754, training accuracy = 0.5862727165222168, testing accuracy = 0.5312864184379578\n",
      "epoch=137/400,0/20 of train, loss=2.3215079307556152, training accuracy = 0.5520000457763672, testing accuracy = 0.5301204919815063\n",
      "epoch=137/400,10/20 of train, loss=2.263378143310547, training accuracy = 0.5900909900665283, testing accuracy = 0.5320637226104736\n",
      "epoch=138/400,0/20 of train, loss=2.376462459564209, training accuracy = 0.49800002574920654, testing accuracy = 0.5308977961540222\n",
      "epoch=138/400,10/20 of train, loss=2.2551534175872803, training accuracy = 0.5963636636734009, testing accuracy = 0.5293431878089905\n",
      "epoch=139/400,0/20 of train, loss=2.340752601623535, training accuracy = 0.5370000004768372, testing accuracy = 0.532452404499054\n",
      "epoch=139/400,10/20 of train, loss=2.2563397884368896, training accuracy = 0.5971818566322327, testing accuracy = 0.5343956351280212\n",
      "epoch=140/400,0/20 of train, loss=2.3138017654418945, training accuracy = 0.5700000524520874, testing accuracy = 0.532452404499054\n",
      "epoch=140/400,10/20 of train, loss=2.259040117263794, training accuracy = 0.5978181958198547, testing accuracy = 0.5293431878089905\n",
      "epoch=141/400,0/20 of train, loss=2.3462653160095215, training accuracy = 0.531000018119812, testing accuracy = 0.5312864184379578\n",
      "epoch=141/400,10/20 of train, loss=2.277940511703491, training accuracy = 0.5936364531517029, testing accuracy = 0.5273999571800232\n",
      "epoch=142/400,0/20 of train, loss=2.334160566329956, training accuracy = 0.5420000553131104, testing accuracy = 0.5293431878089905\n",
      "epoch=142/400,10/20 of train, loss=2.25553297996521, training accuracy = 0.5994546413421631, testing accuracy = 0.5312864184379578\n",
      "epoch=143/400,0/20 of train, loss=2.3640990257263184, training accuracy = 0.5060000419616699, testing accuracy = 0.5340070128440857\n",
      "epoch=143/400,10/20 of train, loss=2.278859853744507, training accuracy = 0.5894546508789062, testing accuracy = 0.5406140685081482\n",
      "epoch=144/400,0/20 of train, loss=2.3618950843811035, training accuracy = 0.5120000243186951, testing accuracy = 0.5277885794639587\n",
      "epoch=144/400,10/20 of train, loss=2.2888152599334717, training accuracy = 0.5926363468170166, testing accuracy = 0.5332297086715698\n",
      "epoch=145/400,0/20 of train, loss=2.329906463623047, training accuracy = 0.5430000424385071, testing accuracy = 0.5308977961540222\n",
      "epoch=145/400,10/20 of train, loss=2.293733596801758, training accuracy = 0.5988182425498962, testing accuracy = 0.5305091738700867\n",
      "epoch=146/400,0/20 of train, loss=2.348116397857666, training accuracy = 0.5240000486373901, testing accuracy = 0.5363389253616333\n",
      "epoch=146/400,10/20 of train, loss=2.2604057788848877, training accuracy = 0.5909091234207153, testing accuracy = 0.5285658836364746\n",
      "epoch=147/400,0/20 of train, loss=2.3335089683532715, training accuracy = 0.5430000424385071, testing accuracy = 0.5270112752914429\n",
      "epoch=147/400,10/20 of train, loss=2.284799575805664, training accuracy = 0.594909131526947, testing accuracy = 0.5367275476455688\n",
      "epoch=148/400,0/20 of train, loss=2.347409248352051, training accuracy = 0.5230000019073486, testing accuracy = 0.5308977961540222\n",
      "epoch=148/400,10/20 of train, loss=2.271404266357422, training accuracy = 0.6021818518638611, testing accuracy = 0.5270112752914429\n",
      "epoch=149/400,0/20 of train, loss=2.3389029502868652, training accuracy = 0.5350000262260437, testing accuracy = 0.5301204919815063\n",
      "epoch=149/400,10/20 of train, loss=2.269317865371704, training accuracy = 0.5933637619018555, testing accuracy = 0.532452404499054\n",
      "epoch=150/400,0/20 of train, loss=2.3358802795410156, training accuracy = 0.5390000343322754, testing accuracy = 0.5207928419113159\n",
      "epoch=150/400,10/20 of train, loss=2.2640221118927, training accuracy = 0.5938182473182678, testing accuracy = 0.5301204919815063\n",
      "epoch=151/400,0/20 of train, loss=2.3527116775512695, training accuracy = 0.5190000534057617, testing accuracy = 0.5297318696975708\n",
      "epoch=151/400,10/20 of train, loss=2.259397029876709, training accuracy = 0.6024545431137085, testing accuracy = 0.5312864184379578\n",
      "epoch=152/400,0/20 of train, loss=2.359283447265625, training accuracy = 0.5130000114440918, testing accuracy = 0.5340070128440857\n",
      "epoch=152/400,10/20 of train, loss=2.2712483406066895, training accuracy = 0.5927273035049438, testing accuracy = 0.5285658836364746\n",
      "epoch=153/400,0/20 of train, loss=2.3571455478668213, training accuracy = 0.5120000243186951, testing accuracy = 0.5254566669464111\n",
      "epoch=153/400,10/20 of train, loss=2.294445276260376, training accuracy = 0.5901818871498108, testing accuracy = 0.5316751003265381\n",
      "epoch=154/400,0/20 of train, loss=2.3314836025238037, training accuracy = 0.5440000295639038, testing accuracy = 0.526233971118927\n",
      "epoch=154/400,10/20 of train, loss=2.289915084838867, training accuracy = 0.5934545397758484, testing accuracy = 0.5273999571800232\n",
      "epoch=155/400,0/20 of train, loss=2.34053635597229, training accuracy = 0.5360000133514404, testing accuracy = 0.5301204919815063\n",
      "epoch=155/400,10/20 of train, loss=2.2850024700164795, training accuracy = 0.5952727198600769, testing accuracy = 0.5301204919815063\n",
      "epoch=156/400,0/20 of train, loss=2.318910837173462, training accuracy = 0.5600000023841858, testing accuracy = 0.5273999571800232\n",
      "epoch=156/400,10/20 of train, loss=2.2408711910247803, training accuracy = 0.6013636589050293, testing accuracy = 0.5305091738700867\n",
      "epoch=157/400,0/20 of train, loss=2.3201656341552734, training accuracy = 0.5520000457763672, testing accuracy = 0.532452404499054\n",
      "epoch=157/400,10/20 of train, loss=2.265212059020996, training accuracy = 0.5964546203613281, testing accuracy = 0.5301204919815063\n",
      "epoch=158/400,0/20 of train, loss=2.324173927307129, training accuracy = 0.550000011920929, testing accuracy = 0.5363389253616333\n",
      "epoch=158/400,10/20 of train, loss=2.2569212913513184, training accuracy = 0.5980909466743469, testing accuracy = 0.5343956351280212\n",
      "epoch=159/400,0/20 of train, loss=2.3095407485961914, training accuracy = 0.5660000443458557, testing accuracy = 0.532452404499054\n",
      "epoch=159/400,10/20 of train, loss=2.290719747543335, training accuracy = 0.5922727584838867, testing accuracy = 0.5336183309555054\n",
      "epoch=160/400,0/20 of train, loss=2.324130058288574, training accuracy = 0.5530000329017639, testing accuracy = 0.5375048518180847\n",
      "epoch=160/400,10/20 of train, loss=2.265495777130127, training accuracy = 0.607090950012207, testing accuracy = 0.5336183309555054\n",
      "epoch=161/400,0/20 of train, loss=2.361290216445923, training accuracy = 0.5130000114440918, testing accuracy = 0.5316751003265381\n",
      "epoch=161/400,10/20 of train, loss=2.285093069076538, training accuracy = 0.5963636636734009, testing accuracy = 0.5239020586013794\n",
      "epoch=162/400,0/20 of train, loss=2.3519132137298584, training accuracy = 0.5200000405311584, testing accuracy = 0.5305091738700867\n",
      "epoch=162/400,10/20 of train, loss=2.25116229057312, training accuracy = 0.5971818566322327, testing accuracy = 0.5312864184379578\n",
      "epoch=163/400,0/20 of train, loss=2.3284685611724854, training accuracy = 0.5450000166893005, testing accuracy = 0.5316751003265381\n",
      "epoch=163/400,10/20 of train, loss=2.267155885696411, training accuracy = 0.5960909128189087, testing accuracy = 0.5223474502563477\n",
      "epoch=164/400,0/20 of train, loss=2.3194048404693604, training accuracy = 0.5509999990463257, testing accuracy = 0.5161290168762207\n",
      "epoch=164/400,10/20 of train, loss=2.2885854244232178, training accuracy = 0.5886364579200745, testing accuracy = 0.5258453488349915\n",
      "epoch=165/400,0/20 of train, loss=2.3557496070861816, training accuracy = 0.5220000147819519, testing accuracy = 0.5340070128440857\n",
      "epoch=165/400,10/20 of train, loss=2.2837607860565186, training accuracy = 0.5942727327346802, testing accuracy = 0.5285658836364746\n",
      "epoch=166/400,0/20 of train, loss=2.3457305431365967, training accuracy = 0.5260000228881836, testing accuracy = 0.5340070128440857\n",
      "epoch=166/400,10/20 of train, loss=2.287517786026001, training accuracy = 0.5922727584838867, testing accuracy = 0.5402254462242126\n",
      "epoch=167/400,0/20 of train, loss=2.3467578887939453, training accuracy = 0.5260000228881836, testing accuracy = 0.5293431878089905\n",
      "epoch=167/400,10/20 of train, loss=2.271974802017212, training accuracy = 0.6008182168006897, testing accuracy = 0.5382821559906006\n",
      "epoch=168/400,0/20 of train, loss=2.3458268642425537, training accuracy = 0.5290000438690186, testing accuracy = 0.5312864184379578\n",
      "epoch=168/400,10/20 of train, loss=2.273674964904785, training accuracy = 0.6007273197174072, testing accuracy = 0.5336183309555054\n",
      "epoch=169/400,0/20 of train, loss=2.337369203567505, training accuracy = 0.5410000085830688, testing accuracy = 0.5328410267829895\n",
      "epoch=169/400,10/20 of train, loss=2.2651519775390625, training accuracy = 0.6004545092582703, testing accuracy = 0.535950243473053\n",
      "epoch=170/400,0/20 of train, loss=2.326584577560425, training accuracy = 0.5480000376701355, testing accuracy = 0.5336183309555054\n",
      "epoch=170/400,10/20 of train, loss=2.2638955116271973, training accuracy = 0.6021818518638611, testing accuracy = 0.5320637226104736\n",
      "epoch=171/400,0/20 of train, loss=2.3234376907348633, training accuracy = 0.5460000038146973, testing accuracy = 0.5308977961540222\n",
      "epoch=171/400,10/20 of train, loss=2.2762246131896973, training accuracy = 0.591090977191925, testing accuracy = 0.5254566669464111\n",
      "epoch=172/400,0/20 of train, loss=2.3508834838867188, training accuracy = 0.5220000147819519, testing accuracy = 0.5250680446624756\n",
      "epoch=172/400,10/20 of train, loss=2.266880989074707, training accuracy = 0.6007273197174072, testing accuracy = 0.5316751003265381\n",
      "epoch=173/400,0/20 of train, loss=2.3503713607788086, training accuracy = 0.5220000147819519, testing accuracy = 0.5363389253616333\n",
      "epoch=173/400,10/20 of train, loss=2.2508928775787354, training accuracy = 0.5960909128189087, testing accuracy = 0.535950243473053\n",
      "epoch=174/400,0/20 of train, loss=2.317318916320801, training accuracy = 0.5540000200271606, testing accuracy = 0.5351729393005371\n",
      "epoch=174/400,10/20 of train, loss=2.2704243659973145, training accuracy = 0.6011819243431091, testing accuracy = 0.532452404499054\n",
      "epoch=175/400,0/20 of train, loss=2.345414638519287, training accuracy = 0.534000039100647, testing accuracy = 0.5312864184379578\n",
      "epoch=175/400,10/20 of train, loss=2.2796452045440674, training accuracy = 0.603727400302887, testing accuracy = 0.5308977961540222\n",
      "epoch=176/400,0/20 of train, loss=2.35971736907959, training accuracy = 0.5139999985694885, testing accuracy = 0.5289545655250549\n",
      "epoch=176/400,10/20 of train, loss=2.2449092864990234, training accuracy = 0.6038182377815247, testing accuracy = 0.5332297086715698\n",
      "epoch=177/400,0/20 of train, loss=2.3333003520965576, training accuracy = 0.5430000424385071, testing accuracy = 0.5297318696975708\n",
      "epoch=177/400,10/20 of train, loss=2.2776100635528564, training accuracy = 0.6075454950332642, testing accuracy = 0.5312864184379578\n",
      "epoch=178/400,0/20 of train, loss=2.3192310333251953, training accuracy = 0.5560000538825989, testing accuracy = 0.5297318696975708\n",
      "epoch=178/400,10/20 of train, loss=2.279879093170166, training accuracy = 0.599818229675293, testing accuracy = 0.5273999571800232\n",
      "epoch=179/400,0/20 of train, loss=2.3163132667541504, training accuracy = 0.5520000457763672, testing accuracy = 0.5312864184379578\n",
      "epoch=179/400,10/20 of train, loss=2.2461295127868652, training accuracy = 0.6027272939682007, testing accuracy = 0.5336183309555054\n",
      "epoch=180/400,0/20 of train, loss=2.306230306625366, training accuracy = 0.5710000395774841, testing accuracy = 0.5316751003265381\n",
      "epoch=180/400,10/20 of train, loss=2.264397382736206, training accuracy = 0.6069999933242798, testing accuracy = 0.5398367643356323\n",
      "epoch=181/400,0/20 of train, loss=2.3149585723876953, training accuracy = 0.5550000071525574, testing accuracy = 0.5390594601631165\n",
      "epoch=181/400,10/20 of train, loss=2.235466480255127, training accuracy = 0.6133636236190796, testing accuracy = 0.535950243473053\n",
      "epoch=182/400,0/20 of train, loss=2.3476030826568604, training accuracy = 0.5200000405311584, testing accuracy = 0.526233971118927\n",
      "epoch=182/400,10/20 of train, loss=2.2832696437835693, training accuracy = 0.5976364016532898, testing accuracy = 0.5386708378791809\n",
      "epoch=183/400,0/20 of train, loss=2.3295955657958984, training accuracy = 0.5430000424385071, testing accuracy = 0.5340070128440857\n",
      "epoch=183/400,10/20 of train, loss=2.268498659133911, training accuracy = 0.6086363792419434, testing accuracy = 0.5390594601631165\n",
      "epoch=184/400,0/20 of train, loss=2.329174757003784, training accuracy = 0.5480000376701355, testing accuracy = 0.5406140685081482\n",
      "epoch=184/400,10/20 of train, loss=2.234612464904785, training accuracy = 0.6076364517211914, testing accuracy = 0.5386708378791809\n",
      "epoch=185/400,0/20 of train, loss=2.3528311252593994, training accuracy = 0.5200000405311584, testing accuracy = 0.532452404499054\n",
      "epoch=185/400,10/20 of train, loss=2.2776288986206055, training accuracy = 0.5978181958198547, testing accuracy = 0.5410027503967285\n",
      "epoch=186/400,0/20 of train, loss=2.3241846561431885, training accuracy = 0.5530000329017639, testing accuracy = 0.535950243473053\n",
      "epoch=186/400,10/20 of train, loss=2.2468345165252686, training accuracy = 0.6044546365737915, testing accuracy = 0.5390594601631165\n",
      "epoch=187/400,0/20 of train, loss=2.321343421936035, training accuracy = 0.5520000457763672, testing accuracy = 0.5398367643356323\n",
      "epoch=187/400,10/20 of train, loss=2.2576441764831543, training accuracy = 0.6104545593261719, testing accuracy = 0.5371162295341492\n",
      "epoch=188/400,0/20 of train, loss=2.3531250953674316, training accuracy = 0.5180000066757202, testing accuracy = 0.532452404499054\n",
      "epoch=188/400,10/20 of train, loss=2.2285454273223877, training accuracy = 0.6040909886360168, testing accuracy = 0.5308977961540222\n",
      "epoch=189/400,0/20 of train, loss=2.339848518371582, training accuracy = 0.5370000004768372, testing accuracy = 0.5336183309555054\n",
      "epoch=189/400,10/20 of train, loss=2.256817579269409, training accuracy = 0.6021819114685059, testing accuracy = 0.537893533706665\n",
      "epoch=190/400,0/20 of train, loss=2.3463048934936523, training accuracy = 0.5290000438690186, testing accuracy = 0.5448892712593079\n",
      "epoch=190/400,10/20 of train, loss=2.270815134048462, training accuracy = 0.6128182411193848, testing accuracy = 0.535950243473053\n",
      "epoch=191/400,0/20 of train, loss=2.341850757598877, training accuracy = 0.534000039100647, testing accuracy = 0.5390594601631165\n",
      "epoch=191/400,10/20 of train, loss=2.280796527862549, training accuracy = 0.60345458984375, testing accuracy = 0.5371162295341492\n",
      "epoch=192/400,0/20 of train, loss=2.3352203369140625, training accuracy = 0.5410000085830688, testing accuracy = 0.5402254462242126\n",
      "epoch=192/400,10/20 of train, loss=2.242690324783325, training accuracy = 0.6138182282447815, testing accuracy = 0.5390594601631165\n",
      "epoch=193/400,0/20 of train, loss=2.3337066173553467, training accuracy = 0.5420000553131104, testing accuracy = 0.5433346629142761\n",
      "epoch=193/400,10/20 of train, loss=2.2410528659820557, training accuracy = 0.6067273616790771, testing accuracy = 0.535950243473053\n",
      "epoch=194/400,0/20 of train, loss=2.340707302093506, training accuracy = 0.5350000262260437, testing accuracy = 0.535950243473053\n",
      "epoch=194/400,10/20 of train, loss=2.264141321182251, training accuracy = 0.6102727055549622, testing accuracy = 0.5289545655250549\n",
      "epoch=195/400,0/20 of train, loss=2.3425893783569336, training accuracy = 0.531000018119812, testing accuracy = 0.5351729393005371\n",
      "epoch=195/400,10/20 of train, loss=2.2489190101623535, training accuracy = 0.6117272973060608, testing accuracy = 0.5417800545692444\n",
      "epoch=196/400,0/20 of train, loss=2.3255820274353027, training accuracy = 0.5460000038146973, testing accuracy = 0.5390594601631165\n",
      "epoch=196/400,10/20 of train, loss=2.284912109375, training accuracy = 0.6099091172218323, testing accuracy = 0.5367275476455688\n",
      "epoch=197/400,0/20 of train, loss=2.3278114795684814, training accuracy = 0.5470000505447388, testing accuracy = 0.5332297086715698\n",
      "epoch=197/400,10/20 of train, loss=2.2577812671661377, training accuracy = 0.6016364097595215, testing accuracy = 0.5382821559906006\n",
      "epoch=198/400,0/20 of train, loss=2.3397834300994873, training accuracy = 0.531000018119812, testing accuracy = 0.532452404499054\n",
      "epoch=198/400,10/20 of train, loss=2.247523546218872, training accuracy = 0.6156363487243652, testing accuracy = 0.5425573587417603\n",
      "epoch=199/400,0/20 of train, loss=2.328977346420288, training accuracy = 0.5450000166893005, testing accuracy = 0.5371162295341492\n",
      "epoch=199/400,10/20 of train, loss=2.2256317138671875, training accuracy = 0.611818253993988, testing accuracy = 0.5312864184379578\n",
      "epoch=200/400,0/20 of train, loss=2.2997844219207764, training accuracy = 0.5770000219345093, testing accuracy = 0.5320637226104736\n",
      "epoch=200/400,10/20 of train, loss=2.263528823852539, training accuracy = 0.619454562664032, testing accuracy = 0.5398367643356323\n",
      "epoch=201/400,0/20 of train, loss=2.324713945388794, training accuracy = 0.5509999990463257, testing accuracy = 0.5386708378791809\n",
      "epoch=201/400,10/20 of train, loss=2.258929967880249, training accuracy = 0.6185454726219177, testing accuracy = 0.5340070128440857\n",
      "epoch=202/400,0/20 of train, loss=2.3365869522094727, training accuracy = 0.534000039100647, testing accuracy = 0.5386708378791809\n",
      "epoch=202/400,10/20 of train, loss=2.2782442569732666, training accuracy = 0.609000027179718, testing accuracy = 0.5425573587417603\n",
      "epoch=203/400,0/20 of train, loss=2.3447084426879883, training accuracy = 0.5290000438690186, testing accuracy = 0.5386708378791809\n",
      "epoch=203/400,10/20 of train, loss=2.2634735107421875, training accuracy = 0.610090970993042, testing accuracy = 0.5336183309555054\n",
      "epoch=204/400,0/20 of train, loss=2.33354115486145, training accuracy = 0.5400000214576721, testing accuracy = 0.5363389253616333\n",
      "epoch=204/400,10/20 of train, loss=2.2489869594573975, training accuracy = 0.6092727780342102, testing accuracy = 0.5390594601631165\n",
      "epoch=205/400,0/20 of train, loss=2.3245022296905518, training accuracy = 0.550000011920929, testing accuracy = 0.5316751003265381\n",
      "epoch=205/400,10/20 of train, loss=2.284104585647583, training accuracy = 0.6040909886360168, testing accuracy = 0.5375048518180847\n",
      "epoch=206/400,0/20 of train, loss=2.3203177452087402, training accuracy = 0.5530000329017639, testing accuracy = 0.5343956351280212\n",
      "epoch=206/400,10/20 of train, loss=2.292245864868164, training accuracy = 0.610909104347229, testing accuracy = 0.5363389253616333\n",
      "epoch=207/400,0/20 of train, loss=2.338040351867676, training accuracy = 0.5420000553131104, testing accuracy = 0.5320637226104736\n",
      "epoch=207/400,10/20 of train, loss=2.2460238933563232, training accuracy = 0.6144545674324036, testing accuracy = 0.5382821559906006\n",
      "epoch=208/400,0/20 of train, loss=2.3430426120758057, training accuracy = 0.534000039100647, testing accuracy = 0.5340070128440857\n",
      "epoch=208/400,10/20 of train, loss=2.2471182346343994, training accuracy = 0.6154546141624451, testing accuracy = 0.5293431878089905\n",
      "epoch=209/400,0/20 of train, loss=2.3550097942352295, training accuracy = 0.5160000324249268, testing accuracy = 0.535950243473053\n",
      "epoch=209/400,10/20 of train, loss=2.280345916748047, training accuracy = 0.6144545674324036, testing accuracy = 0.5367275476455688\n",
      "epoch=210/400,0/20 of train, loss=2.3270092010498047, training accuracy = 0.5470000505447388, testing accuracy = 0.5382821559906006\n",
      "epoch=210/400,10/20 of train, loss=2.250244617462158, training accuracy = 0.6140909790992737, testing accuracy = 0.5312864184379578\n",
      "epoch=211/400,0/20 of train, loss=2.35575532913208, training accuracy = 0.5200000405311584, testing accuracy = 0.5320637226104736\n",
      "epoch=211/400,10/20 of train, loss=2.246793746948242, training accuracy = 0.610909104347229, testing accuracy = 0.5375048518180847\n",
      "epoch=212/400,0/20 of train, loss=2.3251914978027344, training accuracy = 0.5450000166893005, testing accuracy = 0.532452404499054\n",
      "epoch=212/400,10/20 of train, loss=2.2693233489990234, training accuracy = 0.6193637251853943, testing accuracy = 0.535950243473053\n",
      "epoch=213/400,0/20 of train, loss=2.3499767780303955, training accuracy = 0.5200000405311584, testing accuracy = 0.5316751003265381\n",
      "epoch=213/400,10/20 of train, loss=2.2689199447631836, training accuracy = 0.6141818761825562, testing accuracy = 0.5406140685081482\n",
      "epoch=214/400,0/20 of train, loss=2.332883358001709, training accuracy = 0.5410000085830688, testing accuracy = 0.5390594601631165\n",
      "epoch=214/400,10/20 of train, loss=2.22989559173584, training accuracy = 0.6184545755386353, testing accuracy = 0.5398367643356323\n",
      "epoch=215/400,0/20 of train, loss=2.331488847732544, training accuracy = 0.5390000343322754, testing accuracy = 0.5394481420516968\n",
      "epoch=215/400,10/20 of train, loss=2.2726926803588867, training accuracy = 0.6151819229125977, testing accuracy = 0.5347843170166016\n",
      "epoch=216/400,0/20 of train, loss=2.346766233444214, training accuracy = 0.5260000228881836, testing accuracy = 0.5371162295341492\n",
      "epoch=216/400,10/20 of train, loss=2.2484757900238037, training accuracy = 0.614545464515686, testing accuracy = 0.5406140685081482\n",
      "epoch=217/400,0/20 of train, loss=2.323209047317505, training accuracy = 0.5470000505447388, testing accuracy = 0.532452404499054\n",
      "epoch=217/400,10/20 of train, loss=2.2703588008880615, training accuracy = 0.6187273263931274, testing accuracy = 0.5410027503967285\n",
      "epoch=218/400,0/20 of train, loss=2.315228223800659, training accuracy = 0.5610000491142273, testing accuracy = 0.5367275476455688\n",
      "epoch=218/400,10/20 of train, loss=2.2604751586914062, training accuracy = 0.6139091849327087, testing accuracy = 0.5390594601631165\n",
      "epoch=219/400,0/20 of train, loss=2.3424811363220215, training accuracy = 0.531000018119812, testing accuracy = 0.5363389253616333\n",
      "epoch=219/400,10/20 of train, loss=2.259052038192749, training accuracy = 0.6082727909088135, testing accuracy = 0.537893533706665\n",
      "epoch=220/400,0/20 of train, loss=2.3341832160949707, training accuracy = 0.5400000214576721, testing accuracy = 0.535950243473053\n",
      "epoch=220/400,10/20 of train, loss=2.2340128421783447, training accuracy = 0.6178181767463684, testing accuracy = 0.537893533706665\n",
      "epoch=221/400,0/20 of train, loss=2.327842950820923, training accuracy = 0.5470000505447388, testing accuracy = 0.5417800545692444\n",
      "epoch=221/400,10/20 of train, loss=2.2489492893218994, training accuracy = 0.6170000433921814, testing accuracy = 0.5398367643356323\n",
      "epoch=222/400,0/20 of train, loss=2.327594757080078, training accuracy = 0.5470000505447388, testing accuracy = 0.5410027503967285\n",
      "epoch=222/400,10/20 of train, loss=2.235004186630249, training accuracy = 0.6225455403327942, testing accuracy = 0.5406140685081482\n",
      "epoch=223/400,0/20 of train, loss=2.310307741165161, training accuracy = 0.5610000491142273, testing accuracy = 0.5394481420516968\n",
      "epoch=223/400,10/20 of train, loss=2.25813889503479, training accuracy = 0.6263635754585266, testing accuracy = 0.5355616211891174\n",
      "epoch=224/400,0/20 of train, loss=2.3424148559570312, training accuracy = 0.531000018119812, testing accuracy = 0.5332297086715698\n",
      "epoch=224/400,10/20 of train, loss=2.254469394683838, training accuracy = 0.6201818585395813, testing accuracy = 0.5375048518180847\n",
      "epoch=225/400,0/20 of train, loss=2.2922110557556152, training accuracy = 0.5840000510215759, testing accuracy = 0.5301204919815063\n",
      "epoch=225/400,10/20 of train, loss=2.238825798034668, training accuracy = 0.6198182106018066, testing accuracy = 0.5390594601631165\n",
      "epoch=226/400,0/20 of train, loss=2.30560564994812, training accuracy = 0.5660000443458557, testing accuracy = 0.5402254462242126\n",
      "epoch=226/400,10/20 of train, loss=2.2673723697662354, training accuracy = 0.6124546527862549, testing accuracy = 0.5406140685081482\n",
      "epoch=227/400,0/20 of train, loss=2.3435630798339844, training accuracy = 0.5290000438690186, testing accuracy = 0.5425573587417603\n",
      "epoch=227/400,10/20 of train, loss=2.2614686489105225, training accuracy = 0.6159999966621399, testing accuracy = 0.5410027503967285\n",
      "epoch=228/400,0/20 of train, loss=2.3435330390930176, training accuracy = 0.5320000052452087, testing accuracy = 0.5328410267829895\n",
      "epoch=228/400,10/20 of train, loss=2.2597811222076416, training accuracy = 0.6136364340782166, testing accuracy = 0.5382821559906006\n",
      "epoch=229/400,0/20 of train, loss=2.3235695362091064, training accuracy = 0.5460000038146973, testing accuracy = 0.5367275476455688\n",
      "epoch=229/400,10/20 of train, loss=2.247812032699585, training accuracy = 0.6237273216247559, testing accuracy = 0.5371162295341492\n",
      "epoch=230/400,0/20 of train, loss=2.329606771469116, training accuracy = 0.5470000505447388, testing accuracy = 0.5340070128440857\n",
      "epoch=230/400,10/20 of train, loss=2.265794515609741, training accuracy = 0.6238182187080383, testing accuracy = 0.5402254462242126\n",
      "epoch=231/400,0/20 of train, loss=2.335045337677002, training accuracy = 0.5360000133514404, testing accuracy = 0.5417800545692444\n",
      "epoch=231/400,10/20 of train, loss=2.263157606124878, training accuracy = 0.618636429309845, testing accuracy = 0.5371162295341492\n",
      "epoch=232/400,0/20 of train, loss=2.328465461730957, training accuracy = 0.5430000424385071, testing accuracy = 0.5386708378791809\n",
      "epoch=232/400,10/20 of train, loss=2.254148006439209, training accuracy = 0.6148182153701782, testing accuracy = 0.5375048518180847\n",
      "epoch=233/400,0/20 of train, loss=2.348421335220337, training accuracy = 0.5260000228881836, testing accuracy = 0.5305091738700867\n",
      "epoch=233/400,10/20 of train, loss=2.232537269592285, training accuracy = 0.6200909614562988, testing accuracy = 0.5394481420516968\n",
      "epoch=234/400,0/20 of train, loss=2.318795919418335, training accuracy = 0.5570000410079956, testing accuracy = 0.5382821559906006\n",
      "epoch=234/400,10/20 of train, loss=2.244843006134033, training accuracy = 0.6227272748947144, testing accuracy = 0.5386708378791809\n",
      "epoch=235/400,0/20 of train, loss=2.3350894451141357, training accuracy = 0.5390000343322754, testing accuracy = 0.5343956351280212\n",
      "epoch=235/400,10/20 of train, loss=2.2693185806274414, training accuracy = 0.6165454387664795, testing accuracy = 0.5406140685081482\n",
      "epoch=236/400,0/20 of train, loss=2.2956466674804688, training accuracy = 0.5850000381469727, testing accuracy = 0.5328410267829895\n",
      "epoch=236/400,10/20 of train, loss=2.2476987838745117, training accuracy = 0.6197272539138794, testing accuracy = 0.5386708378791809\n",
      "epoch=237/400,0/20 of train, loss=2.3143887519836426, training accuracy = 0.5610000491142273, testing accuracy = 0.5382821559906006\n",
      "epoch=237/400,10/20 of train, loss=2.241586446762085, training accuracy = 0.6262727975845337, testing accuracy = 0.5402254462242126\n",
      "epoch=238/400,0/20 of train, loss=2.3173575401306152, training accuracy = 0.5580000281333923, testing accuracy = 0.5413913726806641\n",
      "epoch=238/400,10/20 of train, loss=2.253474712371826, training accuracy = 0.6239091157913208, testing accuracy = 0.5367275476455688\n",
      "epoch=239/400,0/20 of train, loss=2.3141520023345947, training accuracy = 0.5580000281333923, testing accuracy = 0.5363389253616333\n",
      "epoch=239/400,10/20 of train, loss=2.257296562194824, training accuracy = 0.625909149646759, testing accuracy = 0.5402254462242126\n",
      "epoch=240/400,0/20 of train, loss=2.341423749923706, training accuracy = 0.5360000133514404, testing accuracy = 0.5355616211891174\n",
      "epoch=240/400,10/20 of train, loss=2.235102653503418, training accuracy = 0.6210910081863403, testing accuracy = 0.5308977961540222\n",
      "epoch=241/400,0/20 of train, loss=2.332388162612915, training accuracy = 0.5430000424385071, testing accuracy = 0.5390594601631165\n",
      "epoch=241/400,10/20 of train, loss=2.230630874633789, training accuracy = 0.6264545321464539, testing accuracy = 0.5375048518180847\n",
      "epoch=242/400,0/20 of train, loss=2.333561658859253, training accuracy = 0.5410000085830688, testing accuracy = 0.5375048518180847\n",
      "epoch=242/400,10/20 of train, loss=2.256016492843628, training accuracy = 0.6280000805854797, testing accuracy = 0.5382821559906006\n",
      "epoch=243/400,0/20 of train, loss=2.349029302597046, training accuracy = 0.5240000486373901, testing accuracy = 0.5402254462242126\n",
      "epoch=243/400,10/20 of train, loss=2.2525546550750732, training accuracy = 0.6219091415405273, testing accuracy = 0.5382821559906006\n",
      "epoch=244/400,0/20 of train, loss=2.343531847000122, training accuracy = 0.5330000519752502, testing accuracy = 0.5316751003265381\n",
      "epoch=244/400,10/20 of train, loss=2.2443668842315674, training accuracy = 0.6308181881904602, testing accuracy = 0.5394481420516968\n",
      "epoch=245/400,0/20 of train, loss=2.3264400959014893, training accuracy = 0.5509999990463257, testing accuracy = 0.5382821559906006\n",
      "epoch=245/400,10/20 of train, loss=2.2504100799560547, training accuracy = 0.6249091625213623, testing accuracy = 0.5390594601631165\n",
      "epoch=246/400,0/20 of train, loss=2.3181798458099365, training accuracy = 0.5600000023841858, testing accuracy = 0.5410027503967285\n",
      "epoch=246/400,10/20 of train, loss=2.2144219875335693, training accuracy = 0.6330000758171082, testing accuracy = 0.5413913726806641\n",
      "epoch=247/400,0/20 of train, loss=2.322758436203003, training accuracy = 0.5490000247955322, testing accuracy = 0.535950243473053\n",
      "epoch=247/400,10/20 of train, loss=2.265770673751831, training accuracy = 0.621454656124115, testing accuracy = 0.5371162295341492\n",
      "epoch=248/400,0/20 of train, loss=2.3121001720428467, training accuracy = 0.5630000233650208, testing accuracy = 0.5301204919815063\n",
      "epoch=248/400,10/20 of train, loss=2.2657852172851562, training accuracy = 0.6252727508544922, testing accuracy = 0.5425573587417603\n",
      "epoch=249/400,0/20 of train, loss=2.3510870933532715, training accuracy = 0.5210000276565552, testing accuracy = 0.537893533706665\n",
      "epoch=249/400,10/20 of train, loss=2.2630553245544434, training accuracy = 0.6276363730430603, testing accuracy = 0.5367275476455688\n",
      "epoch=250/400,0/20 of train, loss=2.3327853679656982, training accuracy = 0.5390000343322754, testing accuracy = 0.532452404499054\n",
      "epoch=250/400,10/20 of train, loss=2.2462563514709473, training accuracy = 0.6146363615989685, testing accuracy = 0.5301204919815063\n",
      "epoch=251/400,0/20 of train, loss=2.34698224067688, training accuracy = 0.5260000228881836, testing accuracy = 0.5343956351280212\n",
      "epoch=251/400,10/20 of train, loss=2.217393159866333, training accuracy = 0.6224546432495117, testing accuracy = 0.5363389253616333\n",
      "epoch=252/400,0/20 of train, loss=2.3576438426971436, training accuracy = 0.5170000195503235, testing accuracy = 0.5351729393005371\n",
      "epoch=252/400,10/20 of train, loss=2.23407244682312, training accuracy = 0.6258181929588318, testing accuracy = 0.5421686768531799\n",
      "epoch=253/400,0/20 of train, loss=2.309570074081421, training accuracy = 0.5649999976158142, testing accuracy = 0.5375048518180847\n",
      "epoch=253/400,10/20 of train, loss=2.249380588531494, training accuracy = 0.6307273507118225, testing accuracy = 0.5343956351280212\n",
      "epoch=254/400,0/20 of train, loss=2.3274478912353516, training accuracy = 0.5520000457763672, testing accuracy = 0.5340070128440857\n",
      "epoch=254/400,10/20 of train, loss=2.242115020751953, training accuracy = 0.6310909390449524, testing accuracy = 0.5382821559906006\n",
      "epoch=255/400,0/20 of train, loss=2.3237619400024414, training accuracy = 0.5509999990463257, testing accuracy = 0.5351729393005371\n",
      "epoch=255/400,10/20 of train, loss=2.2399332523345947, training accuracy = 0.6230000853538513, testing accuracy = 0.5343956351280212\n",
      "epoch=256/400,0/20 of train, loss=2.3147706985473633, training accuracy = 0.5610000491142273, testing accuracy = 0.5347843170166016\n",
      "epoch=256/400,10/20 of train, loss=2.2330877780914307, training accuracy = 0.622272789478302, testing accuracy = 0.5340070128440857\n",
      "epoch=257/400,0/20 of train, loss=2.346872329711914, training accuracy = 0.5250000357627869, testing accuracy = 0.5413913726806641\n",
      "epoch=257/400,10/20 of train, loss=2.2383413314819336, training accuracy = 0.6238182783126831, testing accuracy = 0.535950243473053\n",
      "epoch=258/400,0/20 of train, loss=2.3284549713134766, training accuracy = 0.5450000166893005, testing accuracy = 0.5390594601631165\n",
      "epoch=258/400,10/20 of train, loss=2.2496531009674072, training accuracy = 0.6228181719779968, testing accuracy = 0.5390594601631165\n",
      "epoch=259/400,0/20 of train, loss=2.3196167945861816, training accuracy = 0.5550000071525574, testing accuracy = 0.5371162295341492\n",
      "epoch=259/400,10/20 of train, loss=2.2641427516937256, training accuracy = 0.6307272911071777, testing accuracy = 0.5312864184379578\n",
      "epoch=260/400,0/20 of train, loss=2.3418407440185547, training accuracy = 0.5300000309944153, testing accuracy = 0.5347843170166016\n",
      "epoch=260/400,10/20 of train, loss=2.257315158843994, training accuracy = 0.6275454759597778, testing accuracy = 0.5351729393005371\n",
      "epoch=261/400,0/20 of train, loss=2.304330587387085, training accuracy = 0.5730000138282776, testing accuracy = 0.5413913726806641\n",
      "epoch=261/400,10/20 of train, loss=2.2074134349823, training accuracy = 0.6294545531272888, testing accuracy = 0.5371162295341492\n",
      "epoch=262/400,0/20 of train, loss=2.328073501586914, training accuracy = 0.5450000166893005, testing accuracy = 0.537893533706665\n",
      "epoch=262/400,10/20 of train, loss=2.264504909515381, training accuracy = 0.6210908889770508, testing accuracy = 0.5347843170166016\n",
      "epoch=263/400,0/20 of train, loss=2.329092502593994, training accuracy = 0.5460000038146973, testing accuracy = 0.532452404499054\n",
      "epoch=263/400,10/20 of train, loss=2.2455153465270996, training accuracy = 0.633363664150238, testing accuracy = 0.5406140685081482\n",
      "epoch=264/400,0/20 of train, loss=2.2976086139678955, training accuracy = 0.581000030040741, testing accuracy = 0.5371162295341492\n",
      "epoch=264/400,10/20 of train, loss=2.2671470642089844, training accuracy = 0.6271818280220032, testing accuracy = 0.5398367643356323\n",
      "epoch=265/400,0/20 of train, loss=2.3464314937591553, training accuracy = 0.5250000357627869, testing accuracy = 0.535950243473053\n",
      "epoch=265/400,10/20 of train, loss=2.23654842376709, training accuracy = 0.6250910758972168, testing accuracy = 0.5386708378791809\n",
      "epoch=266/400,0/20 of train, loss=2.3438332080841064, training accuracy = 0.5270000100135803, testing accuracy = 0.5375048518180847\n",
      "epoch=266/400,10/20 of train, loss=2.255324363708496, training accuracy = 0.619727373123169, testing accuracy = 0.5375048518180847\n",
      "epoch=267/400,0/20 of train, loss=2.3268325328826904, training accuracy = 0.5490000247955322, testing accuracy = 0.544111967086792\n",
      "epoch=267/400,10/20 of train, loss=2.242429733276367, training accuracy = 0.6258182525634766, testing accuracy = 0.5343956351280212\n",
      "epoch=268/400,0/20 of train, loss=2.324439525604248, training accuracy = 0.5520000457763672, testing accuracy = 0.5343956351280212\n",
      "epoch=268/400,10/20 of train, loss=2.2527213096618652, training accuracy = 0.6216363906860352, testing accuracy = 0.5363389253616333\n",
      "epoch=269/400,0/20 of train, loss=2.3305623531341553, training accuracy = 0.5480000376701355, testing accuracy = 0.5347843170166016\n",
      "epoch=269/400,10/20 of train, loss=2.2497737407684326, training accuracy = 0.6283636689186096, testing accuracy = 0.5410027503967285\n",
      "epoch=270/400,0/20 of train, loss=2.364084482192993, training accuracy = 0.5110000371932983, testing accuracy = 0.5336183309555054\n",
      "epoch=270/400,10/20 of train, loss=2.2613139152526855, training accuracy = 0.6283636689186096, testing accuracy = 0.5297318696975708\n",
      "epoch=271/400,0/20 of train, loss=2.3403780460357666, training accuracy = 0.5330000519752502, testing accuracy = 0.5386708378791809\n",
      "epoch=271/400,10/20 of train, loss=2.2411978244781494, training accuracy = 0.6349091529846191, testing accuracy = 0.537893533706665\n",
      "epoch=272/400,0/20 of train, loss=2.3155908584594727, training accuracy = 0.5570000410079956, testing accuracy = 0.5355616211891174\n",
      "epoch=272/400,10/20 of train, loss=2.2528443336486816, training accuracy = 0.6307272911071777, testing accuracy = 0.5417800545692444\n",
      "epoch=273/400,0/20 of train, loss=2.3496828079223633, training accuracy = 0.5210000276565552, testing accuracy = 0.5336183309555054\n",
      "epoch=273/400,10/20 of train, loss=2.2616162300109863, training accuracy = 0.6235455274581909, testing accuracy = 0.5375048518180847\n",
      "epoch=274/400,0/20 of train, loss=2.3266875743865967, training accuracy = 0.5480000376701355, testing accuracy = 0.5413913726806641\n",
      "epoch=274/400,10/20 of train, loss=2.2398149967193604, training accuracy = 0.6327272653579712, testing accuracy = 0.5347843170166016\n",
      "epoch=275/400,0/20 of train, loss=2.3347866535186768, training accuracy = 0.5390000343322754, testing accuracy = 0.5343956351280212\n",
      "epoch=275/400,10/20 of train, loss=2.227294445037842, training accuracy = 0.6294546723365784, testing accuracy = 0.5402254462242126\n",
      "epoch=276/400,0/20 of train, loss=2.323629856109619, training accuracy = 0.550000011920929, testing accuracy = 0.5340070128440857\n",
      "epoch=276/400,10/20 of train, loss=2.22308087348938, training accuracy = 0.6321818828582764, testing accuracy = 0.5343956351280212\n",
      "epoch=277/400,0/20 of train, loss=2.311805009841919, training accuracy = 0.5600000023841858, testing accuracy = 0.5402254462242126\n",
      "epoch=277/400,10/20 of train, loss=2.218494176864624, training accuracy = 0.6349091529846191, testing accuracy = 0.5406140685081482\n",
      "epoch=278/400,0/20 of train, loss=2.3226099014282227, training accuracy = 0.5509999990463257, testing accuracy = 0.5375048518180847\n",
      "epoch=278/400,10/20 of train, loss=2.2351458072662354, training accuracy = 0.6348182559013367, testing accuracy = 0.5410027503967285\n",
      "epoch=279/400,0/20 of train, loss=2.340822219848633, training accuracy = 0.5300000309944153, testing accuracy = 0.5297318696975708\n",
      "epoch=279/400,10/20 of train, loss=2.230628490447998, training accuracy = 0.6244545578956604, testing accuracy = 0.5406140685081482\n",
      "epoch=280/400,0/20 of train, loss=2.3162453174591064, training accuracy = 0.5600000023841858, testing accuracy = 0.5328410267829895\n",
      "epoch=280/400,10/20 of train, loss=2.2116169929504395, training accuracy = 0.6338182091712952, testing accuracy = 0.5390594601631165\n",
      "epoch=281/400,0/20 of train, loss=2.3315489292144775, training accuracy = 0.5430000424385071, testing accuracy = 0.5410027503967285\n",
      "epoch=281/400,10/20 of train, loss=2.251431703567505, training accuracy = 0.634636402130127, testing accuracy = 0.5363389253616333\n",
      "epoch=282/400,0/20 of train, loss=2.337425470352173, training accuracy = 0.5370000004768372, testing accuracy = 0.5367275476455688\n",
      "epoch=282/400,10/20 of train, loss=2.2502732276916504, training accuracy = 0.6235455274581909, testing accuracy = 0.5363389253616333\n",
      "epoch=283/400,0/20 of train, loss=2.332657814025879, training accuracy = 0.5400000214576721, testing accuracy = 0.535950243473053\n",
      "epoch=283/400,10/20 of train, loss=2.2268354892730713, training accuracy = 0.6293637156486511, testing accuracy = 0.5382821559906006\n",
      "epoch=284/400,0/20 of train, loss=2.307199478149414, training accuracy = 0.5630000233650208, testing accuracy = 0.537893533706665\n",
      "epoch=284/400,10/20 of train, loss=2.2164039611816406, training accuracy = 0.6325454711914062, testing accuracy = 0.5394481420516968\n",
      "epoch=285/400,0/20 of train, loss=2.3354196548461914, training accuracy = 0.5390000343322754, testing accuracy = 0.5390594601631165\n",
      "epoch=285/400,10/20 of train, loss=2.2338380813598633, training accuracy = 0.6338182687759399, testing accuracy = 0.5347843170166016\n",
      "epoch=286/400,0/20 of train, loss=2.321174383163452, training accuracy = 0.550000011920929, testing accuracy = 0.5417800545692444\n",
      "epoch=286/400,10/20 of train, loss=2.259021520614624, training accuracy = 0.6292727589607239, testing accuracy = 0.5413913726806641\n",
      "epoch=287/400,0/20 of train, loss=2.311997890472412, training accuracy = 0.5660000443458557, testing accuracy = 0.5386708378791809\n",
      "epoch=287/400,10/20 of train, loss=2.22198748588562, training accuracy = 0.6341818571090698, testing accuracy = 0.537893533706665\n",
      "epoch=288/400,0/20 of train, loss=2.3167080879211426, training accuracy = 0.5580000281333923, testing accuracy = 0.5394481420516968\n",
      "epoch=288/400,10/20 of train, loss=2.2338526248931885, training accuracy = 0.6361818313598633, testing accuracy = 0.5332297086715698\n",
      "epoch=289/400,0/20 of train, loss=2.3626420497894287, training accuracy = 0.5090000033378601, testing accuracy = 0.5340070128440857\n",
      "epoch=289/400,10/20 of train, loss=2.236079692840576, training accuracy = 0.626727283000946, testing accuracy = 0.537893533706665\n",
      "epoch=290/400,0/20 of train, loss=2.3085882663726807, training accuracy = 0.5640000104904175, testing accuracy = 0.5417800545692444\n",
      "epoch=290/400,10/20 of train, loss=2.2397348880767822, training accuracy = 0.6310909390449524, testing accuracy = 0.5351729393005371\n",
      "epoch=291/400,0/20 of train, loss=2.3110151290893555, training accuracy = 0.5640000104904175, testing accuracy = 0.5363389253616333\n",
      "epoch=291/400,10/20 of train, loss=2.2283668518066406, training accuracy = 0.6305454969406128, testing accuracy = 0.5371162295341492\n",
      "epoch=292/400,0/20 of train, loss=2.325176477432251, training accuracy = 0.5450000166893005, testing accuracy = 0.5343956351280212\n",
      "epoch=292/400,10/20 of train, loss=2.2389185428619385, training accuracy = 0.6306363940238953, testing accuracy = 0.5355616211891174\n",
      "epoch=293/400,0/20 of train, loss=2.3492283821105957, training accuracy = 0.5250000357627869, testing accuracy = 0.537893533706665\n",
      "epoch=293/400,10/20 of train, loss=2.2487568855285645, training accuracy = 0.6246364116668701, testing accuracy = 0.5386708378791809\n",
      "epoch=294/400,0/20 of train, loss=2.344289779663086, training accuracy = 0.527999997138977, testing accuracy = 0.5355616211891174\n",
      "epoch=294/400,10/20 of train, loss=2.226996660232544, training accuracy = 0.6309090852737427, testing accuracy = 0.5336183309555054\n",
      "epoch=295/400,0/20 of train, loss=2.328235626220703, training accuracy = 0.5450000166893005, testing accuracy = 0.5351729393005371\n",
      "epoch=295/400,10/20 of train, loss=2.236436605453491, training accuracy = 0.6257273554801941, testing accuracy = 0.5305091738700867\n",
      "epoch=296/400,0/20 of train, loss=2.3188116550445557, training accuracy = 0.5550000071525574, testing accuracy = 0.5410027503967285\n",
      "epoch=296/400,10/20 of train, loss=2.2349088191986084, training accuracy = 0.6276364326477051, testing accuracy = 0.5351729393005371\n",
      "epoch=297/400,0/20 of train, loss=2.301987409591675, training accuracy = 0.5720000267028809, testing accuracy = 0.535950243473053\n",
      "epoch=297/400,10/20 of train, loss=2.2201757431030273, training accuracy = 0.6377273201942444, testing accuracy = 0.5402254462242126\n",
      "epoch=298/400,0/20 of train, loss=2.3387155532836914, training accuracy = 0.534000039100647, testing accuracy = 0.5351729393005371\n",
      "epoch=298/400,10/20 of train, loss=2.2219629287719727, training accuracy = 0.6277273297309875, testing accuracy = 0.5343956351280212\n",
      "epoch=299/400,0/20 of train, loss=2.3036389350891113, training accuracy = 0.5720000267028809, testing accuracy = 0.5402254462242126\n",
      "epoch=299/400,10/20 of train, loss=2.24595308303833, training accuracy = 0.6324545741081238, testing accuracy = 0.5375048518180847\n",
      "epoch=300/400,0/20 of train, loss=2.331035852432251, training accuracy = 0.5450000166893005, testing accuracy = 0.5367275476455688\n",
      "epoch=300/400,10/20 of train, loss=2.2463457584381104, training accuracy = 0.6382728219032288, testing accuracy = 0.5394481420516968\n",
      "epoch=301/400,0/20 of train, loss=2.313279390335083, training accuracy = 0.5590000152587891, testing accuracy = 0.5433346629142761\n",
      "epoch=301/400,10/20 of train, loss=2.2447938919067383, training accuracy = 0.6349091529846191, testing accuracy = 0.5386708378791809\n",
      "epoch=302/400,0/20 of train, loss=2.310572385787964, training accuracy = 0.562000036239624, testing accuracy = 0.5371162295341492\n",
      "epoch=302/400,10/20 of train, loss=2.2269155979156494, training accuracy = 0.6399090886116028, testing accuracy = 0.5351729393005371\n",
      "epoch=303/400,0/20 of train, loss=2.355698823928833, training accuracy = 0.5190000534057617, testing accuracy = 0.5343956351280212\n",
      "epoch=303/400,10/20 of train, loss=2.260225296020508, training accuracy = 0.6352728009223938, testing accuracy = 0.5343956351280212\n",
      "epoch=304/400,0/20 of train, loss=2.329838275909424, training accuracy = 0.5450000166893005, testing accuracy = 0.5394481420516968\n",
      "epoch=304/400,10/20 of train, loss=2.2360284328460693, training accuracy = 0.638090968132019, testing accuracy = 0.5343956351280212\n",
      "epoch=305/400,0/20 of train, loss=2.305645227432251, training accuracy = 0.5710000395774841, testing accuracy = 0.5390594601631165\n",
      "epoch=305/400,10/20 of train, loss=2.240513563156128, training accuracy = 0.6360000371932983, testing accuracy = 0.5375048518180847\n",
      "epoch=306/400,0/20 of train, loss=2.315582036972046, training accuracy = 0.5580000281333923, testing accuracy = 0.5343956351280212\n",
      "epoch=306/400,10/20 of train, loss=2.2426724433898926, training accuracy = 0.6364545822143555, testing accuracy = 0.5371162295341492\n",
      "epoch=307/400,0/20 of train, loss=2.3218419551849365, training accuracy = 0.5590000152587891, testing accuracy = 0.5394481420516968\n",
      "epoch=307/400,10/20 of train, loss=2.205958604812622, training accuracy = 0.6326364278793335, testing accuracy = 0.5336183309555054\n",
      "epoch=308/400,0/20 of train, loss=2.318352222442627, training accuracy = 0.5520000457763672, testing accuracy = 0.5394481420516968\n",
      "epoch=308/400,10/20 of train, loss=2.232522487640381, training accuracy = 0.6356363892555237, testing accuracy = 0.5343956351280212\n",
      "epoch=309/400,0/20 of train, loss=2.322375774383545, training accuracy = 0.5490000247955322, testing accuracy = 0.5382821559906006\n",
      "epoch=309/400,10/20 of train, loss=2.251091480255127, training accuracy = 0.6393637657165527, testing accuracy = 0.5351729393005371\n",
      "epoch=310/400,0/20 of train, loss=2.3346822261810303, training accuracy = 0.5380000472068787, testing accuracy = 0.5375048518180847\n",
      "epoch=310/400,10/20 of train, loss=2.2377448081970215, training accuracy = 0.6374545693397522, testing accuracy = 0.5394481420516968\n",
      "epoch=311/400,0/20 of train, loss=2.313305377960205, training accuracy = 0.562000036239624, testing accuracy = 0.5363389253616333\n",
      "epoch=311/400,10/20 of train, loss=2.223051071166992, training accuracy = 0.6293637752532959, testing accuracy = 0.5347843170166016\n",
      "epoch=312/400,0/20 of train, loss=2.3105971813201904, training accuracy = 0.5610000491142273, testing accuracy = 0.5402254462242126\n",
      "epoch=312/400,10/20 of train, loss=2.228842258453369, training accuracy = 0.6368182301521301, testing accuracy = 0.5343956351280212\n",
      "epoch=313/400,0/20 of train, loss=2.32600474357605, training accuracy = 0.550000011920929, testing accuracy = 0.532452404499054\n",
      "epoch=313/400,10/20 of train, loss=2.2522623538970947, training accuracy = 0.6251819133758545, testing accuracy = 0.532452404499054\n",
      "epoch=314/400,0/20 of train, loss=2.31982421875, training accuracy = 0.5610000491142273, testing accuracy = 0.5316751003265381\n",
      "epoch=314/400,10/20 of train, loss=2.240442991256714, training accuracy = 0.6367273330688477, testing accuracy = 0.5367275476455688\n",
      "epoch=315/400,0/20 of train, loss=2.321608543395996, training accuracy = 0.5540000200271606, testing accuracy = 0.5355616211891174\n",
      "epoch=315/400,10/20 of train, loss=2.2134170532226562, training accuracy = 0.6396363973617554, testing accuracy = 0.5340070128440857\n",
      "epoch=316/400,0/20 of train, loss=2.3136565685272217, training accuracy = 0.5600000023841858, testing accuracy = 0.5371162295341492\n",
      "epoch=316/400,10/20 of train, loss=2.2323505878448486, training accuracy = 0.634636402130127, testing accuracy = 0.537893533706665\n",
      "epoch=317/400,0/20 of train, loss=2.3262383937835693, training accuracy = 0.550000011920929, testing accuracy = 0.5305091738700867\n",
      "epoch=317/400,10/20 of train, loss=2.241840362548828, training accuracy = 0.6250909566879272, testing accuracy = 0.5340070128440857\n",
      "epoch=318/400,0/20 of train, loss=2.3221652507781982, training accuracy = 0.5550000071525574, testing accuracy = 0.5336183309555054\n",
      "epoch=318/400,10/20 of train, loss=2.232175350189209, training accuracy = 0.6377273797988892, testing accuracy = 0.5394481420516968\n",
      "epoch=319/400,0/20 of train, loss=2.3038315773010254, training accuracy = 0.5710000395774841, testing accuracy = 0.5351729393005371\n",
      "epoch=319/400,10/20 of train, loss=2.242706537246704, training accuracy = 0.6340909600257874, testing accuracy = 0.5336183309555054\n",
      "epoch=320/400,0/20 of train, loss=2.3366153240203857, training accuracy = 0.5370000004768372, testing accuracy = 0.526233971118927\n",
      "epoch=320/400,10/20 of train, loss=2.223766326904297, training accuracy = 0.6252727508544922, testing accuracy = 0.535950243473053\n",
      "epoch=321/400,0/20 of train, loss=2.3216710090637207, training accuracy = 0.550000011920929, testing accuracy = 0.5347843170166016\n",
      "epoch=321/400,10/20 of train, loss=2.252264976501465, training accuracy = 0.6209090948104858, testing accuracy = 0.5402254462242126\n",
      "epoch=322/400,0/20 of train, loss=2.319423198699951, training accuracy = 0.5540000200271606, testing accuracy = 0.5390594601631165\n",
      "epoch=322/400,10/20 of train, loss=2.236309766769409, training accuracy = 0.6319090723991394, testing accuracy = 0.5332297086715698\n",
      "epoch=323/400,0/20 of train, loss=2.318842887878418, training accuracy = 0.5570000410079956, testing accuracy = 0.537893533706665\n",
      "epoch=323/400,10/20 of train, loss=2.2579119205474854, training accuracy = 0.6388182044029236, testing accuracy = 0.5355616211891174\n",
      "epoch=324/400,0/20 of train, loss=2.308999538421631, training accuracy = 0.5630000233650208, testing accuracy = 0.5390594601631165\n",
      "epoch=324/400,10/20 of train, loss=2.2269372940063477, training accuracy = 0.6428182721138, testing accuracy = 0.5371162295341492\n",
      "epoch=325/400,0/20 of train, loss=2.2984790802001953, training accuracy = 0.5760000348091125, testing accuracy = 0.5398367643356323\n",
      "epoch=325/400,10/20 of train, loss=2.2528858184814453, training accuracy = 0.6379091143608093, testing accuracy = 0.5398367643356323\n",
      "epoch=326/400,0/20 of train, loss=2.332425832748413, training accuracy = 0.5400000214576721, testing accuracy = 0.5371162295341492\n",
      "epoch=326/400,10/20 of train, loss=2.2473320960998535, training accuracy = 0.6331818699836731, testing accuracy = 0.5375048518180847\n",
      "epoch=327/400,0/20 of train, loss=2.3078413009643555, training accuracy = 0.5670000314712524, testing accuracy = 0.5351729393005371\n",
      "epoch=327/400,10/20 of train, loss=2.235063314437866, training accuracy = 0.6356363892555237, testing accuracy = 0.535950243473053\n",
      "epoch=328/400,0/20 of train, loss=2.316314935684204, training accuracy = 0.5570000410079956, testing accuracy = 0.5410027503967285\n",
      "epoch=328/400,10/20 of train, loss=2.2253520488739014, training accuracy = 0.6387273073196411, testing accuracy = 0.5363389253616333\n",
      "epoch=329/400,0/20 of train, loss=2.3229246139526367, training accuracy = 0.550000011920929, testing accuracy = 0.5363389253616333\n",
      "epoch=329/400,10/20 of train, loss=2.226252555847168, training accuracy = 0.6439091563224792, testing accuracy = 0.5394481420516968\n",
      "epoch=330/400,0/20 of train, loss=2.3237202167510986, training accuracy = 0.5509999990463257, testing accuracy = 0.5343956351280212\n",
      "epoch=330/400,10/20 of train, loss=2.226348638534546, training accuracy = 0.6343636512756348, testing accuracy = 0.5382821559906006\n",
      "epoch=331/400,0/20 of train, loss=2.3088278770446777, training accuracy = 0.5660000443458557, testing accuracy = 0.5371162295341492\n",
      "epoch=331/400,10/20 of train, loss=2.2398459911346436, training accuracy = 0.6416363716125488, testing accuracy = 0.5347843170166016\n",
      "epoch=332/400,0/20 of train, loss=2.314939022064209, training accuracy = 0.5590000152587891, testing accuracy = 0.5394481420516968\n",
      "epoch=332/400,10/20 of train, loss=2.209479331970215, training accuracy = 0.6391818523406982, testing accuracy = 0.5363389253616333\n",
      "epoch=333/400,0/20 of train, loss=2.32804274559021, training accuracy = 0.5450000166893005, testing accuracy = 0.5417800545692444\n",
      "epoch=333/400,10/20 of train, loss=2.233433485031128, training accuracy = 0.632727324962616, testing accuracy = 0.5371162295341492\n",
      "epoch=334/400,0/20 of train, loss=2.31770396232605, training accuracy = 0.5560000538825989, testing accuracy = 0.5328410267829895\n",
      "epoch=334/400,10/20 of train, loss=2.208346366882324, training accuracy = 0.6446363925933838, testing accuracy = 0.5351729393005371\n",
      "epoch=335/400,0/20 of train, loss=2.3207926750183105, training accuracy = 0.5480000376701355, testing accuracy = 0.5375048518180847\n",
      "epoch=335/400,10/20 of train, loss=2.2592451572418213, training accuracy = 0.6329091191291809, testing accuracy = 0.5316751003265381\n",
      "epoch=336/400,0/20 of train, loss=2.30798602104187, training accuracy = 0.5640000104904175, testing accuracy = 0.5363389253616333\n",
      "epoch=336/400,10/20 of train, loss=2.2754454612731934, training accuracy = 0.6384545564651489, testing accuracy = 0.5367275476455688\n",
      "epoch=337/400,0/20 of train, loss=2.313521146774292, training accuracy = 0.5590000152587891, testing accuracy = 0.5347843170166016\n",
      "epoch=337/400,10/20 of train, loss=2.2283828258514404, training accuracy = 0.6381818652153015, testing accuracy = 0.5316751003265381\n",
      "epoch=338/400,0/20 of train, loss=2.3119826316833496, training accuracy = 0.5590000152587891, testing accuracy = 0.535950243473053\n",
      "epoch=338/400,10/20 of train, loss=2.2353692054748535, training accuracy = 0.63718181848526, testing accuracy = 0.537893533706665\n",
      "epoch=339/400,0/20 of train, loss=2.3273608684539795, training accuracy = 0.5440000295639038, testing accuracy = 0.5390594601631165\n",
      "epoch=339/400,10/20 of train, loss=2.2499752044677734, training accuracy = 0.6273636817932129, testing accuracy = 0.5413913726806641\n",
      "epoch=340/400,0/20 of train, loss=2.318027973175049, training accuracy = 0.5530000329017639, testing accuracy = 0.5343956351280212\n",
      "epoch=340/400,10/20 of train, loss=2.255812883377075, training accuracy = 0.6343636512756348, testing accuracy = 0.5394481420516968\n",
      "epoch=341/400,0/20 of train, loss=2.312058687210083, training accuracy = 0.5630000233650208, testing accuracy = 0.5386708378791809\n",
      "epoch=341/400,10/20 of train, loss=2.2383882999420166, training accuracy = 0.6397272944450378, testing accuracy = 0.5382821559906006\n",
      "epoch=342/400,0/20 of train, loss=2.3295304775238037, training accuracy = 0.5440000295639038, testing accuracy = 0.537893533706665\n",
      "epoch=342/400,10/20 of train, loss=2.2277395725250244, training accuracy = 0.6390001177787781, testing accuracy = 0.5371162295341492\n",
      "epoch=343/400,0/20 of train, loss=2.325631618499756, training accuracy = 0.5470000505447388, testing accuracy = 0.5363389253616333\n",
      "epoch=343/400,10/20 of train, loss=2.20271635055542, training accuracy = 0.6358181238174438, testing accuracy = 0.5355616211891174\n",
      "epoch=344/400,0/20 of train, loss=2.3152830600738525, training accuracy = 0.5640000104904175, testing accuracy = 0.5301204919815063\n",
      "epoch=344/400,10/20 of train, loss=2.2301063537597656, training accuracy = 0.6370000243186951, testing accuracy = 0.5394481420516968\n",
      "epoch=345/400,0/20 of train, loss=2.2924821376800537, training accuracy = 0.581000030040741, testing accuracy = 0.537893533706665\n",
      "epoch=345/400,10/20 of train, loss=2.2170023918151855, training accuracy = 0.643818199634552, testing accuracy = 0.5390594601631165\n",
      "epoch=346/400,0/20 of train, loss=2.3202829360961914, training accuracy = 0.5540000200271606, testing accuracy = 0.5363389253616333\n",
      "epoch=346/400,10/20 of train, loss=2.2274231910705566, training accuracy = 0.643818199634552, testing accuracy = 0.5386708378791809\n",
      "epoch=347/400,0/20 of train, loss=2.3074119091033936, training accuracy = 0.5630000233650208, testing accuracy = 0.5355616211891174\n",
      "epoch=347/400,10/20 of train, loss=2.2120895385742188, training accuracy = 0.6469090580940247, testing accuracy = 0.5355616211891174\n",
      "epoch=348/400,0/20 of train, loss=2.318239688873291, training accuracy = 0.5570000410079956, testing accuracy = 0.537893533706665\n",
      "epoch=348/400,10/20 of train, loss=2.214984893798828, training accuracy = 0.6386364102363586, testing accuracy = 0.5413913726806641\n",
      "epoch=349/400,0/20 of train, loss=2.344815492630005, training accuracy = 0.5290000438690186, testing accuracy = 0.5355616211891174\n",
      "epoch=349/400,10/20 of train, loss=2.2535743713378906, training accuracy = 0.6377272605895996, testing accuracy = 0.5371162295341492\n",
      "epoch=350/400,0/20 of train, loss=2.3124406337738037, training accuracy = 0.5630000233650208, testing accuracy = 0.5394481420516968\n",
      "epoch=350/400,10/20 of train, loss=2.220417022705078, training accuracy = 0.6406364440917969, testing accuracy = 0.537893533706665\n",
      "epoch=351/400,0/20 of train, loss=2.2978687286376953, training accuracy = 0.5750000476837158, testing accuracy = 0.5320637226104736\n",
      "epoch=351/400,10/20 of train, loss=2.1917388439178467, training accuracy = 0.6406364440917969, testing accuracy = 0.5406140685081482\n",
      "epoch=352/400,0/20 of train, loss=2.306168794631958, training accuracy = 0.5649999976158142, testing accuracy = 0.5355616211891174\n",
      "epoch=352/400,10/20 of train, loss=2.232187032699585, training accuracy = 0.6328182220458984, testing accuracy = 0.5312864184379578\n",
      "epoch=353/400,0/20 of train, loss=2.3254613876342773, training accuracy = 0.5480000376701355, testing accuracy = 0.532452404499054\n",
      "epoch=353/400,10/20 of train, loss=2.2299633026123047, training accuracy = 0.6352728009223938, testing accuracy = 0.5332297086715698\n",
      "epoch=354/400,0/20 of train, loss=2.324934244155884, training accuracy = 0.5470000505447388, testing accuracy = 0.5382821559906006\n",
      "epoch=354/400,10/20 of train, loss=2.211235523223877, training accuracy = 0.6384546160697937, testing accuracy = 0.5347843170166016\n",
      "epoch=355/400,0/20 of train, loss=2.303539276123047, training accuracy = 0.5700000524520874, testing accuracy = 0.5308977961540222\n",
      "epoch=355/400,10/20 of train, loss=2.2301573753356934, training accuracy = 0.6409091949462891, testing accuracy = 0.5402254462242126\n",
      "epoch=356/400,0/20 of train, loss=2.324530839920044, training accuracy = 0.5490000247955322, testing accuracy = 0.5343956351280212\n",
      "epoch=356/400,10/20 of train, loss=2.2235512733459473, training accuracy = 0.64000004529953, testing accuracy = 0.5340070128440857\n",
      "epoch=357/400,0/20 of train, loss=2.342266321182251, training accuracy = 0.5290000438690186, testing accuracy = 0.5425573587417603\n",
      "epoch=357/400,10/20 of train, loss=2.2278037071228027, training accuracy = 0.6420909762382507, testing accuracy = 0.5316751003265381\n",
      "epoch=358/400,0/20 of train, loss=2.336280107498169, training accuracy = 0.5370000004768372, testing accuracy = 0.5382821559906006\n",
      "epoch=358/400,10/20 of train, loss=2.2100284099578857, training accuracy = 0.6404545903205872, testing accuracy = 0.537893533706665\n",
      "epoch=359/400,0/20 of train, loss=2.3080596923828125, training accuracy = 0.5649999976158142, testing accuracy = 0.5382821559906006\n",
      "epoch=359/400,10/20 of train, loss=2.2411460876464844, training accuracy = 0.6426363587379456, testing accuracy = 0.5386708378791809\n",
      "epoch=360/400,0/20 of train, loss=2.308157205581665, training accuracy = 0.5649999976158142, testing accuracy = 0.537893533706665\n",
      "epoch=360/400,10/20 of train, loss=2.219017505645752, training accuracy = 0.6420000195503235, testing accuracy = 0.5445005893707275\n",
      "epoch=361/400,0/20 of train, loss=2.3016510009765625, training accuracy = 0.5690000057220459, testing accuracy = 0.5355616211891174\n",
      "epoch=361/400,10/20 of train, loss=2.223733425140381, training accuracy = 0.6465454697608948, testing accuracy = 0.5406140685081482\n",
      "epoch=362/400,0/20 of train, loss=2.307358980178833, training accuracy = 0.5649999976158142, testing accuracy = 0.5347843170166016\n",
      "epoch=362/400,10/20 of train, loss=2.2288906574249268, training accuracy = 0.6384546160697937, testing accuracy = 0.5413913726806641\n",
      "epoch=363/400,0/20 of train, loss=2.314250946044922, training accuracy = 0.5649999976158142, testing accuracy = 0.5351729393005371\n",
      "epoch=363/400,10/20 of train, loss=2.2426514625549316, training accuracy = 0.6494545936584473, testing accuracy = 0.5398367643356323\n",
      "epoch=364/400,0/20 of train, loss=2.315013885498047, training accuracy = 0.5580000281333923, testing accuracy = 0.5375048518180847\n",
      "epoch=364/400,10/20 of train, loss=2.232158899307251, training accuracy = 0.6416363716125488, testing accuracy = 0.5328410267829895\n",
      "epoch=365/400,0/20 of train, loss=2.3224806785583496, training accuracy = 0.5509999990463257, testing accuracy = 0.5382821559906006\n",
      "epoch=365/400,10/20 of train, loss=2.2170045375823975, training accuracy = 0.6323636770248413, testing accuracy = 0.5382821559906006\n",
      "epoch=366/400,0/20 of train, loss=2.3120474815368652, training accuracy = 0.5610000491142273, testing accuracy = 0.5386708378791809\n",
      "epoch=366/400,10/20 of train, loss=2.2330431938171387, training accuracy = 0.6451818346977234, testing accuracy = 0.537893533706665\n",
      "epoch=367/400,0/20 of train, loss=2.294711112976074, training accuracy = 0.5800000429153442, testing accuracy = 0.5332297086715698\n",
      "epoch=367/400,10/20 of train, loss=2.224761962890625, training accuracy = 0.6462727189064026, testing accuracy = 0.537893533706665\n",
      "epoch=368/400,0/20 of train, loss=2.317504644393921, training accuracy = 0.5560000538825989, testing accuracy = 0.5351729393005371\n",
      "epoch=368/400,10/20 of train, loss=2.226327657699585, training accuracy = 0.639363706111908, testing accuracy = 0.5429459810256958\n",
      "epoch=369/400,0/20 of train, loss=2.3256430625915527, training accuracy = 0.5490000247955322, testing accuracy = 0.5343956351280212\n",
      "epoch=369/400,10/20 of train, loss=2.256592273712158, training accuracy = 0.6404546499252319, testing accuracy = 0.5394481420516968\n",
      "epoch=370/400,0/20 of train, loss=2.3021645545959473, training accuracy = 0.5730000138282776, testing accuracy = 0.5367275476455688\n",
      "epoch=370/400,10/20 of train, loss=2.2494149208068848, training accuracy = 0.6351818442344666, testing accuracy = 0.532452404499054\n",
      "epoch=371/400,0/20 of train, loss=2.298299551010132, training accuracy = 0.5750000476837158, testing accuracy = 0.5340070128440857\n",
      "epoch=371/400,10/20 of train, loss=2.227278709411621, training accuracy = 0.6427273154258728, testing accuracy = 0.5406140685081482\n",
      "epoch=372/400,0/20 of train, loss=2.3273990154266357, training accuracy = 0.5440000295639038, testing accuracy = 0.5347843170166016\n",
      "epoch=372/400,10/20 of train, loss=2.2059152126312256, training accuracy = 0.6384546160697937, testing accuracy = 0.532452404499054\n",
      "epoch=373/400,0/20 of train, loss=2.3189051151275635, training accuracy = 0.5560000538825989, testing accuracy = 0.5371162295341492\n",
      "epoch=373/400,10/20 of train, loss=2.224132537841797, training accuracy = 0.6416364312171936, testing accuracy = 0.5308977961540222\n",
      "epoch=374/400,0/20 of train, loss=2.3437678813934326, training accuracy = 0.531000018119812, testing accuracy = 0.5347843170166016\n",
      "epoch=374/400,10/20 of train, loss=2.221004009246826, training accuracy = 0.6340000033378601, testing accuracy = 0.5394481420516968\n",
      "epoch=375/400,0/20 of train, loss=2.2947843074798584, training accuracy = 0.5790000557899475, testing accuracy = 0.5390594601631165\n",
      "epoch=375/400,10/20 of train, loss=2.1950860023498535, training accuracy = 0.644727349281311, testing accuracy = 0.532452404499054\n",
      "epoch=376/400,0/20 of train, loss=2.32253360748291, training accuracy = 0.5540000200271606, testing accuracy = 0.5394481420516968\n",
      "epoch=376/400,10/20 of train, loss=2.200464963912964, training accuracy = 0.6394546031951904, testing accuracy = 0.5336183309555054\n",
      "epoch=377/400,0/20 of train, loss=2.324221611022949, training accuracy = 0.5520000457763672, testing accuracy = 0.5367275476455688\n",
      "epoch=377/400,10/20 of train, loss=2.2224600315093994, training accuracy = 0.6450001001358032, testing accuracy = 0.5347843170166016\n",
      "epoch=378/400,0/20 of train, loss=2.2962894439697266, training accuracy = 0.5800000429153442, testing accuracy = 0.5375048518180847\n",
      "epoch=378/400,10/20 of train, loss=2.223313570022583, training accuracy = 0.6439999938011169, testing accuracy = 0.5367275476455688\n",
      "epoch=379/400,0/20 of train, loss=2.3190433979034424, training accuracy = 0.5509999990463257, testing accuracy = 0.5375048518180847\n",
      "epoch=379/400,10/20 of train, loss=2.211358070373535, training accuracy = 0.6447272896766663, testing accuracy = 0.535950243473053\n",
      "epoch=380/400,0/20 of train, loss=2.3208820819854736, training accuracy = 0.5520000457763672, testing accuracy = 0.5413913726806641\n",
      "epoch=380/400,10/20 of train, loss=2.2496984004974365, training accuracy = 0.640818178653717, testing accuracy = 0.5382821559906006\n",
      "epoch=381/400,0/20 of train, loss=2.3338840007781982, training accuracy = 0.5420000553131104, testing accuracy = 0.5328410267829895\n",
      "epoch=381/400,10/20 of train, loss=2.2201802730560303, training accuracy = 0.6326363682746887, testing accuracy = 0.5398367643356323\n",
      "epoch=382/400,0/20 of train, loss=2.3389668464660645, training accuracy = 0.534000039100647, testing accuracy = 0.535950243473053\n",
      "epoch=382/400,10/20 of train, loss=2.239595413208008, training accuracy = 0.64654541015625, testing accuracy = 0.5355616211891174\n",
      "epoch=383/400,0/20 of train, loss=2.3159327507019043, training accuracy = 0.5590000152587891, testing accuracy = 0.5398367643356323\n",
      "epoch=383/400,10/20 of train, loss=2.217160701751709, training accuracy = 0.6416363716125488, testing accuracy = 0.5367275476455688\n",
      "epoch=384/400,0/20 of train, loss=2.3071045875549316, training accuracy = 0.5690000057220459, testing accuracy = 0.5375048518180847\n",
      "epoch=384/400,10/20 of train, loss=2.2440860271453857, training accuracy = 0.6392728090286255, testing accuracy = 0.5347843170166016\n",
      "epoch=385/400,0/20 of train, loss=2.3438467979431152, training accuracy = 0.5300000309944153, testing accuracy = 0.537893533706665\n",
      "epoch=385/400,10/20 of train, loss=2.2137553691864014, training accuracy = 0.6381818652153015, testing accuracy = 0.5367275476455688\n",
      "epoch=386/400,0/20 of train, loss=2.322296380996704, training accuracy = 0.5509999990463257, testing accuracy = 0.5371162295341492\n",
      "epoch=386/400,10/20 of train, loss=2.246671438217163, training accuracy = 0.6369091868400574, testing accuracy = 0.5402254462242126\n",
      "epoch=387/400,0/20 of train, loss=2.335925817489624, training accuracy = 0.5380000472068787, testing accuracy = 0.5363389253616333\n",
      "epoch=387/400,10/20 of train, loss=2.2230958938598633, training accuracy = 0.6397273540496826, testing accuracy = 0.5390594601631165\n",
      "epoch=388/400,0/20 of train, loss=2.326509714126587, training accuracy = 0.5450000166893005, testing accuracy = 0.5367275476455688\n",
      "epoch=388/400,10/20 of train, loss=2.234140396118164, training accuracy = 0.6371818780899048, testing accuracy = 0.5394481420516968\n",
      "epoch=389/400,0/20 of train, loss=2.31463623046875, training accuracy = 0.5600000023841858, testing accuracy = 0.5351729393005371\n",
      "epoch=389/400,10/20 of train, loss=2.2268214225769043, training accuracy = 0.6470910310745239, testing accuracy = 0.5351729393005371\n",
      "epoch=390/400,0/20 of train, loss=2.317481517791748, training accuracy = 0.5530000329017639, testing accuracy = 0.5390594601631165\n",
      "epoch=390/400,10/20 of train, loss=2.214398145675659, training accuracy = 0.6413636803627014, testing accuracy = 0.5340070128440857\n",
      "epoch=391/400,0/20 of train, loss=2.341505527496338, training accuracy = 0.5350000262260437, testing accuracy = 0.5367275476455688\n",
      "epoch=391/400,10/20 of train, loss=2.2034122943878174, training accuracy = 0.6475454568862915, testing accuracy = 0.5375048518180847\n",
      "epoch=392/400,0/20 of train, loss=2.3359439373016357, training accuracy = 0.5380000472068787, testing accuracy = 0.5402254462242126\n",
      "epoch=392/400,10/20 of train, loss=2.2203009128570557, training accuracy = 0.6440000534057617, testing accuracy = 0.5351729393005371\n",
      "epoch=393/400,0/20 of train, loss=2.3261148929595947, training accuracy = 0.5450000166893005, testing accuracy = 0.5367275476455688\n",
      "epoch=393/400,10/20 of train, loss=2.23758602142334, training accuracy = 0.645000159740448, testing accuracy = 0.5406140685081482\n",
      "epoch=394/400,0/20 of train, loss=2.3379082679748535, training accuracy = 0.5350000262260437, testing accuracy = 0.5417800545692444\n",
      "epoch=394/400,10/20 of train, loss=2.1978986263275146, training accuracy = 0.6454545855522156, testing accuracy = 0.5363389253616333\n",
      "epoch=395/400,0/20 of train, loss=2.334336519241333, training accuracy = 0.5400000214576721, testing accuracy = 0.5347843170166016\n",
      "epoch=395/400,10/20 of train, loss=2.2127127647399902, training accuracy = 0.6415454745292664, testing accuracy = 0.5363389253616333\n",
      "epoch=396/400,0/20 of train, loss=2.342010259628296, training accuracy = 0.5350000262260437, testing accuracy = 0.5375048518180847\n",
      "epoch=396/400,10/20 of train, loss=2.2158217430114746, training accuracy = 0.6441819071769714, testing accuracy = 0.5363389253616333\n",
      "epoch=397/400,0/20 of train, loss=2.294919967651367, training accuracy = 0.5760000348091125, testing accuracy = 0.5371162295341492\n",
      "epoch=397/400,10/20 of train, loss=2.2148916721343994, training accuracy = 0.6394546031951904, testing accuracy = 0.5375048518180847\n",
      "epoch=398/400,0/20 of train, loss=2.308683156967163, training accuracy = 0.5670000314712524, testing accuracy = 0.5394481420516968\n",
      "epoch=398/400,10/20 of train, loss=2.2026758193969727, training accuracy = 0.6415454745292664, testing accuracy = 0.5382821559906006\n",
      "epoch=399/400,0/20 of train, loss=2.3465359210968018, training accuracy = 0.5260000228881836, testing accuracy = 0.5375048518180847\n",
      "epoch=399/400,10/20 of train, loss=2.2106900215148926, training accuracy = 0.6457273364067078, testing accuracy = 0.5355616211891174\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "model = MLP(140, 16).to(args.device)\n",
    "\n",
    "parameters_to_optimize = filter(lambda x: x.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(parameters_to_optimize, lr =  args.learning_rate, betas = [0.9, 0.999], eps= 0.00000001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "early_stopping = EarlyStopping(patience=args.patience,verbose=True)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_epochs_loss = []\n",
    "valid_epochs_loss = []\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    train_epoch_loss = []\n",
    "    accuracy_sum = 0.0\n",
    "    step = 0.0\n",
    "    for idx,(data_x,data_y) in enumerate(train_dataloader,0):\n",
    "        data_x = data_x.to(torch.float32).to(args.device)\n",
    "        data_y = data_y.to(torch.float32).to(args.device)\n",
    "        outputs = model(data_x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, data_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss.append(loss.item())\n",
    "        train_loss.append(loss.item())\n",
    "        predictions = outputs\n",
    "        correct_predictions = torch.eq(torch.argmax(predictions, axis= 1), torch.argmax(data_y, axis=1))\n",
    "        accuracy = torch.mean(correct_predictions.float())\n",
    "        accuracy_sum += accuracy\n",
    "        step += 1\n",
    "        if idx%(len(train_dataloader)//2)==0:\n",
    "            model.eval()\n",
    "            predictions = model(X_test_tensor.to(args.device))\n",
    "            correct_predictions = torch.eq(torch.argmax(predictions, axis= 1), torch.tensor(y_test_np).to(args.device))\n",
    "            accuracy = torch.mean(correct_predictions.float())\n",
    "            accuracy_list.append(accuracy.cpu().numpy())\n",
    "            print(\"epoch={}/{},{}/{} of train, loss={}, training accuracy = {}, testing accuracy = {}\".format(\n",
    "            epoch, args.epochs, idx, len(train_dataloader),loss.item(), accuracy_sum/step, accuracy))\n",
    "\n",
    "    train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "\n",
    "    #=====================valid============================\n",
    "    # model.eval()\n",
    "    # predictions = model(X_test_tensor.to(args.device))\n",
    "    # correct_predictions = torch.eq(torch.argmax(predictions, axis= 1), torch.tensor(y_test_np).to(args.device))\n",
    "    # accuracy = torch.mean(correct_predictions.float())\n",
    "    # accuracy_list.append(accuracy.cpu().numpy())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001B[36m4.7054\u001B[0m       \u001B[32m0.5084\u001B[0m        \u001B[35m2.8185\u001B[0m  0.3424\n",
      "      2        \u001B[36m2.7075\u001B[0m       0.4670        \u001B[35m2.1922\u001B[0m  0.3253\n",
      "      3        \u001B[36m2.2027\u001B[0m       0.5009        \u001B[35m1.9160\u001B[0m  0.3101\n",
      "      4        \u001B[36m1.9926\u001B[0m       0.5034        \u001B[35m1.8253\u001B[0m  0.3124\n",
      "      5        \u001B[36m1.8968\u001B[0m       \u001B[32m0.5124\u001B[0m        \u001B[35m1.7496\u001B[0m  0.4402\n",
      "      6        \u001B[36m1.8232\u001B[0m       \u001B[32m0.5177\u001B[0m        \u001B[35m1.6931\u001B[0m  0.3063\n",
      "      7        \u001B[36m1.7736\u001B[0m       \u001B[32m0.5237\u001B[0m        \u001B[35m1.6617\u001B[0m  0.3247\n",
      "      8        \u001B[36m1.7371\u001B[0m       \u001B[32m0.5307\u001B[0m        \u001B[35m1.6298\u001B[0m  0.3171\n",
      "      9        \u001B[36m1.7129\u001B[0m       0.5300        \u001B[35m1.6075\u001B[0m  0.3272\n",
      "     10        \u001B[36m1.6946\u001B[0m       \u001B[32m0.5338\u001B[0m        \u001B[35m1.6003\u001B[0m  0.3033\n",
      "     11        \u001B[36m1.6789\u001B[0m       0.5325        \u001B[35m1.5794\u001B[0m  0.2960\n",
      "     12        \u001B[36m1.6609\u001B[0m       \u001B[32m0.5348\u001B[0m        \u001B[35m1.5660\u001B[0m  0.3085\n",
      "     13        \u001B[36m1.6462\u001B[0m       \u001B[32m0.5365\u001B[0m        \u001B[35m1.5553\u001B[0m  0.3137\n",
      "     14        \u001B[36m1.6280\u001B[0m       0.5343        \u001B[35m1.5436\u001B[0m  0.3110\n",
      "     15        \u001B[36m1.6188\u001B[0m       \u001B[32m0.5385\u001B[0m        \u001B[35m1.5333\u001B[0m  0.3196\n",
      "     16        \u001B[36m1.6095\u001B[0m       \u001B[32m0.5405\u001B[0m        \u001B[35m1.5251\u001B[0m  0.3200\n",
      "     17        \u001B[36m1.5949\u001B[0m       \u001B[32m0.5408\u001B[0m        \u001B[35m1.5198\u001B[0m  0.3267\n",
      "     18        \u001B[36m1.5921\u001B[0m       \u001B[32m0.5513\u001B[0m        \u001B[35m1.5048\u001B[0m  0.3372\n",
      "     19        \u001B[36m1.5723\u001B[0m       \u001B[32m0.5541\u001B[0m        \u001B[35m1.4960\u001B[0m  0.2986\n",
      "     20        \u001B[36m1.5713\u001B[0m       \u001B[32m0.5563\u001B[0m        \u001B[35m1.4915\u001B[0m  0.2941\n",
      "     21        \u001B[36m1.5621\u001B[0m       \u001B[32m0.5571\u001B[0m        \u001B[35m1.4843\u001B[0m  0.3212\n",
      "     22        \u001B[36m1.5562\u001B[0m       0.5558        \u001B[35m1.4759\u001B[0m  0.3123\n",
      "     23        \u001B[36m1.5519\u001B[0m       0.5548        \u001B[35m1.4662\u001B[0m  0.3205\n",
      "     24        \u001B[36m1.5366\u001B[0m       0.5553        \u001B[35m1.4609\u001B[0m  0.3034\n",
      "     25        \u001B[36m1.5310\u001B[0m       \u001B[32m0.5614\u001B[0m        \u001B[35m1.4493\u001B[0m  0.3114\n",
      "     26        \u001B[36m1.5191\u001B[0m       0.5601        \u001B[35m1.4464\u001B[0m  0.3235\n",
      "     27        \u001B[36m1.5113\u001B[0m       \u001B[32m0.5709\u001B[0m        \u001B[35m1.4439\u001B[0m  0.3291\n",
      "     28        1.5154       0.5664        \u001B[35m1.4389\u001B[0m  0.3410\n",
      "     29        \u001B[36m1.5112\u001B[0m       \u001B[32m0.5716\u001B[0m        \u001B[35m1.4295\u001B[0m  0.3215\n",
      "     30        \u001B[36m1.5014\u001B[0m       0.5689        \u001B[35m1.4281\u001B[0m  0.3301\n",
      "     31        1.5037       \u001B[32m0.5752\u001B[0m        \u001B[35m1.4218\u001B[0m  0.3281\n",
      "     32        \u001B[36m1.4924\u001B[0m       0.5742        \u001B[35m1.4170\u001B[0m  0.3855\n",
      "     33        \u001B[36m1.4842\u001B[0m       0.5734        \u001B[35m1.4121\u001B[0m  0.3318\n",
      "     34        \u001B[36m1.4741\u001B[0m       \u001B[32m0.5789\u001B[0m        \u001B[35m1.4097\u001B[0m  0.2980\n",
      "     35        \u001B[36m1.4724\u001B[0m       0.5777        \u001B[35m1.4031\u001B[0m  0.3236\n",
      "     36        \u001B[36m1.4719\u001B[0m       0.5772        1.4031  0.3421\n",
      "     37        \u001B[36m1.4643\u001B[0m       0.5787        \u001B[35m1.3993\u001B[0m  0.3109\n",
      "     38        \u001B[36m1.4583\u001B[0m       0.5729        1.4024  0.3127\n",
      "     39        \u001B[36m1.4541\u001B[0m       0.5787        \u001B[35m1.3959\u001B[0m  0.3013\n",
      "     40        \u001B[36m1.4524\u001B[0m       0.5777        1.3959  0.3093\n",
      "     41        1.4545       \u001B[32m0.5809\u001B[0m        \u001B[35m1.3924\u001B[0m  0.3006\n",
      "     42        \u001B[36m1.4513\u001B[0m       0.5779        \u001B[35m1.3879\u001B[0m  0.3187\n",
      "     43        1.4565       \u001B[32m0.5829\u001B[0m        \u001B[35m1.3861\u001B[0m  0.3489\n",
      "     44        \u001B[36m1.4444\u001B[0m       0.5824        \u001B[35m1.3848\u001B[0m  0.5742\n",
      "     45        \u001B[36m1.4363\u001B[0m       0.5774        1.3858  0.4002\n",
      "     46        1.4412       0.5807        \u001B[35m1.3831\u001B[0m  0.3106\n",
      "     47        1.4410       0.5802        \u001B[35m1.3772\u001B[0m  0.3060\n",
      "     48        \u001B[36m1.4338\u001B[0m       0.5812        1.3777  0.3280\n",
      "     49        \u001B[36m1.4272\u001B[0m       0.5827        \u001B[35m1.3738\u001B[0m  0.3184\n",
      "     50        1.4339       \u001B[32m0.5867\u001B[0m        \u001B[35m1.3733\u001B[0m  0.3046\n",
      "     51        1.4353       0.5847        1.3745  0.2999\n",
      "     52        \u001B[36m1.4217\u001B[0m       0.5839        \u001B[35m1.3714\u001B[0m  0.3109\n",
      "     53        1.4254       0.5814        1.3741  0.3187\n",
      "     54        1.4244       0.5844        \u001B[35m1.3657\u001B[0m  0.3083\n",
      "     55        \u001B[36m1.4154\u001B[0m       0.5849        \u001B[35m1.3638\u001B[0m  0.3181\n",
      "     56        \u001B[36m1.4110\u001B[0m       0.5852        1.3660  0.3427\n",
      "     57        1.4138       0.5867        \u001B[35m1.3623\u001B[0m  0.4178\n",
      "     58        \u001B[36m1.4054\u001B[0m       \u001B[32m0.5900\u001B[0m        1.3646  0.3146\n",
      "     59        1.4093       0.5854        \u001B[35m1.3618\u001B[0m  0.2959\n",
      "     60        1.4067       0.5842        \u001B[35m1.3584\u001B[0m  0.3146\n",
      "     61        \u001B[36m1.4016\u001B[0m       0.5849        1.3598  0.3158\n",
      "     62        1.4071       0.5867        1.3603  0.4871\n",
      "     63        \u001B[36m1.3941\u001B[0m       \u001B[32m0.5907\u001B[0m        \u001B[35m1.3538\u001B[0m  0.3133\n",
      "     64        1.3972       0.5897        \u001B[35m1.3528\u001B[0m  0.3108\n",
      "     65        1.3953       0.5907        \u001B[35m1.3494\u001B[0m  0.3037\n",
      "     66        1.3948       0.5892        1.3542  0.3726\n",
      "     67        1.3962       0.5887        1.3513  0.3187\n",
      "     68        1.3947       0.5902        1.3527  0.3160\n",
      "     69        \u001B[36m1.3878\u001B[0m       0.5885        1.3542  0.3039\n",
      "     70        1.3881       \u001B[32m0.5920\u001B[0m        \u001B[35m1.3481\u001B[0m  0.3741\n",
      "     71        \u001B[36m1.3862\u001B[0m       \u001B[32m0.5922\u001B[0m        \u001B[35m1.3468\u001B[0m  0.3182\n",
      "     72        1.3868       0.5920        1.3473  0.3326\n",
      "     73        1.3880       \u001B[32m0.5937\u001B[0m        \u001B[35m1.3443\u001B[0m  0.3227\n",
      "     74        \u001B[36m1.3808\u001B[0m       0.5915        1.3490  0.3254\n",
      "     75        \u001B[36m1.3802\u001B[0m       0.5912        \u001B[35m1.3416\u001B[0m  0.3228\n",
      "     76        \u001B[36m1.3797\u001B[0m       0.5907        1.3463  0.3210\n",
      "     77        \u001B[36m1.3781\u001B[0m       0.5927        1.3435  0.3622\n",
      "     78        1.3788       0.5905        1.3453  0.3246\n",
      "     79        \u001B[36m1.3699\u001B[0m       0.5922        \u001B[35m1.3391\u001B[0m  0.2940\n",
      "     80        \u001B[36m1.3663\u001B[0m       0.5912        1.3423  0.3003\n",
      "     81        1.3697       0.5937        \u001B[35m1.3387\u001B[0m  0.3117\n",
      "     82        \u001B[36m1.3650\u001B[0m       0.5912        1.3416  0.3157\n",
      "     83        1.3675       0.5927        \u001B[35m1.3372\u001B[0m  0.5415\n",
      "     84        1.3678       0.5915        \u001B[35m1.3368\u001B[0m  0.4335\n",
      "     85        1.3660       0.5917        1.3396  0.3220\n",
      "     86        \u001B[36m1.3649\u001B[0m       0.5927        \u001B[35m1.3353\u001B[0m  0.3120\n",
      "     87        \u001B[36m1.3640\u001B[0m       0.5912        \u001B[35m1.3350\u001B[0m  0.3110\n",
      "     88        1.3677       \u001B[32m0.5947\u001B[0m        \u001B[35m1.3343\u001B[0m  0.3092\n",
      "     89        \u001B[36m1.3598\u001B[0m       0.5905        1.3357  0.3224\n",
      "     90        \u001B[36m1.3580\u001B[0m       \u001B[32m0.5965\u001B[0m        \u001B[35m1.3300\u001B[0m  0.3265\n",
      "     91        \u001B[36m1.3561\u001B[0m       0.5932        1.3352  0.3024\n",
      "     92        1.3598       0.5920        1.3343  0.3041\n",
      "     93        \u001B[36m1.3546\u001B[0m       0.5922        1.3320  0.3114\n",
      "     94        \u001B[36m1.3508\u001B[0m       0.5947        1.3310  0.3150\n",
      "     95        1.3518       0.5912        1.3317  0.3003\n",
      "     96        1.3547       0.5935        1.3303  0.3144\n",
      "     97        1.3548       \u001B[32m0.5975\u001B[0m        \u001B[35m1.3264\u001B[0m  0.3524\n",
      "     98        1.3520       0.5940        1.3270  0.3076\n",
      "     99        1.3531       0.5962        \u001B[35m1.3238\u001B[0m  0.3047\n",
      "    100        \u001B[36m1.3431\u001B[0m       0.5967        1.3246  0.3000\n",
      "    101        1.3513       0.5965        \u001B[35m1.3227\u001B[0m  0.3073\n",
      "    102        1.3502       0.5972        1.3227  0.3225\n",
      "    103        1.3537       0.5970        1.3239  0.3091\n",
      "    104        1.3495       0.5975        \u001B[35m1.3222\u001B[0m  0.3003\n",
      "    105        \u001B[36m1.3420\u001B[0m       0.5957        \u001B[35m1.3185\u001B[0m  0.3067\n",
      "    106        \u001B[36m1.3405\u001B[0m       \u001B[32m0.5995\u001B[0m        1.3225  0.3737\n",
      "    107        1.3419       0.5932        1.3251  0.2876\n",
      "    108        \u001B[36m1.3381\u001B[0m       0.5982        1.3199  0.2986\n",
      "    109        \u001B[36m1.3377\u001B[0m       0.5980        \u001B[35m1.3181\u001B[0m  0.2956\n",
      "    110        \u001B[36m1.3345\u001B[0m       0.5947        1.3211  0.3420\n",
      "    111        1.3353       \u001B[32m0.5997\u001B[0m        \u001B[35m1.3175\u001B[0m  0.3258\n",
      "    112        1.3353       0.5975        1.3179  0.3198\n",
      "    113        \u001B[36m1.3330\u001B[0m       0.5985        \u001B[35m1.3172\u001B[0m  0.3018\n",
      "    114        1.3368       \u001B[32m0.6010\u001B[0m        \u001B[35m1.3150\u001B[0m  0.5434\n",
      "    115        1.3358       0.5977        1.3184  0.3360\n",
      "    116        1.3369       0.6003        \u001B[35m1.3144\u001B[0m  0.5942\n",
      "    117        1.3331       \u001B[32m0.6023\u001B[0m        \u001B[35m1.3137\u001B[0m  0.3131\n",
      "    118        1.3383       0.5977        1.3152  0.3069\n",
      "    119        1.3356       0.5995        1.3155  0.2964\n",
      "    120        \u001B[36m1.3312\u001B[0m       0.6015        \u001B[35m1.3124\u001B[0m  0.3278\n",
      "    121        1.3401       0.6013        1.3172  0.4656\n",
      "    122        1.3376       0.5957        1.3136  0.3992\n",
      "    123        1.3434       0.5995        \u001B[35m1.3114\u001B[0m  0.3741\n",
      "    124        \u001B[36m1.3276\u001B[0m       \u001B[32m0.6043\u001B[0m        1.3132  0.3313\n",
      "    125        \u001B[36m1.3264\u001B[0m       0.5997        1.3133  0.3135\n",
      "    126        \u001B[36m1.3226\u001B[0m       0.6003        1.3118  0.3128\n",
      "    127        \u001B[36m1.3215\u001B[0m       0.6015        1.3152  0.2863\n",
      "    128        1.3260       0.6003        \u001B[35m1.3096\u001B[0m  0.3188\n",
      "    129        \u001B[36m1.3212\u001B[0m       0.6008        1.3101  0.3075\n",
      "    130        \u001B[36m1.3204\u001B[0m       \u001B[32m0.6050\u001B[0m        1.3101  0.3134\n",
      "    131        1.3222       0.5995        \u001B[35m1.3060\u001B[0m  0.3230\n",
      "    132        1.3248       \u001B[32m0.6065\u001B[0m        1.3079  0.3513\n",
      "    133        1.3259       0.5965        1.3117  0.3077\n",
      "    134        1.3269       0.5985        1.3176  0.3522\n",
      "    135        1.3231       0.6023        \u001B[35m1.3052\u001B[0m  0.3135\n",
      "    136        1.3272       \u001B[32m0.6075\u001B[0m        1.3078  0.3107\n",
      "    137        \u001B[36m1.3191\u001B[0m       0.6070        1.3066  0.3387\n",
      "    138        1.3253       \u001B[32m0.6095\u001B[0m        1.3076  0.3309\n",
      "    139        \u001B[36m1.3120\u001B[0m       0.6048        1.3109  0.5113\n",
      "    140        1.3183       0.6058        1.3081  0.2993\n",
      "    141        \u001B[36m1.3042\u001B[0m       0.6093        1.3069  0.3070\n",
      "    142        1.3071       \u001B[32m0.6123\u001B[0m        \u001B[35m1.3049\u001B[0m  0.3024\n",
      "    143        \u001B[36m1.2987\u001B[0m       0.6093        1.3073  0.3011\n",
      "    144        1.3123       0.6043        1.3118  0.3187\n",
      "    145        1.3117       0.6053        1.3120  0.3065\n",
      "    146        1.3077       0.6043        1.3107  0.2979\n",
      "    147        1.3132       0.6088        1.3128  0.6547\n",
      "    148        1.3059       0.6120        1.3090  0.3066\n",
      "    149        1.3020       0.6113        1.3075  0.4714\n",
      "    150        1.3064       0.6105        1.3051  0.2999\n",
      "    151        1.3074       0.6090        \u001B[35m1.3034\u001B[0m  0.3003\n",
      "    152        \u001B[36m1.2890\u001B[0m       0.6088        1.3079  0.3188\n",
      "    153        1.3011       \u001B[32m0.6161\u001B[0m        \u001B[35m1.2984\u001B[0m  0.3135\n",
      "    154        1.3009       0.6133        1.3011  0.3044\n",
      "    155        1.3090       0.6108        1.3053  0.3298\n",
      "    156        1.3004       0.6118        1.3049  0.3185\n",
      "    157        1.2894       0.6110        1.3071  0.3308\n",
      "    158        1.2989       0.6123        1.3054  0.3096\n",
      "    159        1.2961       0.6108        1.3011  0.3192\n",
      "    160        1.2989       0.6120        1.3028  0.3118\n",
      "    161        \u001B[36m1.2884\u001B[0m       \u001B[32m0.6178\u001B[0m        \u001B[35m1.2938\u001B[0m  0.3194\n",
      "    162        \u001B[36m1.2848\u001B[0m       0.6100        1.3025  0.3216\n",
      "    163        1.2953       0.6118        1.3001  0.3380\n",
      "    164        1.2943       0.6148        1.2961  0.3014\n",
      "    165        1.2907       0.6143        1.3005  0.3283\n",
      "    166        1.2943       0.6118        1.3061  0.3110\n",
      "    167        1.2899       0.6138        1.3010  0.3253\n",
      "    168        1.2887       0.6141        1.2971  0.3280\n",
      "    169        1.2893       0.6130        1.2998  0.3215\n",
      "    170        \u001B[36m1.2826\u001B[0m       0.6125        1.2953  0.3410\n",
      "    171        1.2871       0.6125        1.2941  0.3295\n",
      "    172        1.2850       0.6153        1.2967  0.3009\n",
      "    173        \u001B[36m1.2811\u001B[0m       0.6151        1.2956  0.3303\n",
      "    174        1.2815       0.6100        1.3000  0.4410\n",
      "    175        1.2849       \u001B[32m0.6183\u001B[0m        1.2942  0.3333\n",
      "    176        \u001B[36m1.2810\u001B[0m       0.6105        1.3037  0.4381\n",
      "    177        1.2834       0.6118        1.2961  0.3275\n",
      "    178        \u001B[36m1.2787\u001B[0m       0.6181        1.2955  0.3387\n",
      "    179        1.2796       0.6153        \u001B[35m1.2892\u001B[0m  0.3157\n",
      "    180        1.2830       0.6148        1.2940  0.3119\n",
      "    181        \u001B[36m1.2776\u001B[0m       0.6141        1.2948  0.3151\n",
      "    182        1.2807       0.6173        1.2919  0.3249\n",
      "    183        \u001B[36m1.2751\u001B[0m       0.6108        1.2929  0.3166\n",
      "    184        \u001B[36m1.2731\u001B[0m       0.6173        1.2916  0.3373\n",
      "    185        \u001B[36m1.2657\u001B[0m       0.6143        1.2920  0.3209\n",
      "    186        1.2744       0.6156        1.2945  0.3303\n",
      "    187        1.2698       0.6151        1.2894  0.3155\n",
      "    188        1.2685       0.6138        1.3010  0.3160\n",
      "    189        1.2758       0.6153        1.2933  0.3082\n",
      "    190        1.2714       \u001B[32m0.6218\u001B[0m        1.2921  0.3178\n",
      "    191        1.2684       0.6183        1.2955  0.3066\n",
      "    192        1.2678       0.6110        1.2950  0.3313\n",
      "    193        1.2686       0.6120        1.2945  0.3215\n",
      "    194        1.2701       0.6188        1.2925  0.3249\n",
      "    195        \u001B[36m1.2608\u001B[0m       0.6153        1.2972  0.3549\n",
      "    196        \u001B[36m1.2590\u001B[0m       0.6201        1.2929  0.3337\n",
      "    197        1.2676       0.6148        1.2994  0.3385\n",
      "    198        1.2692       0.6216        \u001B[35m1.2875\u001B[0m  0.3279\n",
      "    199        1.2642       \u001B[32m0.6223\u001B[0m        1.2881  0.3300\n",
      "    200        1.2645       0.6193        1.2878  0.3118\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.5647104547221142"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "mnn = NeuralNetClassifier(model, max_epochs=200, lr=0.001, batch_size=1000, optimizer=optim.Adam)\n",
    "mnn.fit(X_train_np.astype('float32'), y_train_np)\n",
    "mnn.score(X_test_np.astype('float32'), y_test_np)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "mnn = NeuralNetClassifier(model, max_epochs=200, lr=0.001, batch_size=10000, optimizer=optim.Adam)\n",
    "mnn.fit(X_train_tensor, y_train_np)\n",
    "mnn.score(X_test_tensor, y_test_np)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.1610,  4.0913,  1.3561,  ...,  5.5406,  5.8819,  5.3047],\n        [ 2.9958,  0.6376,  0.2148,  ...,  6.6153,  5.6904,  6.6683],\n        [ 3.9623,  5.9557,  0.6369,  ...,  8.0138,  7.3404,  6.9348],\n        ...,\n        [15.3014,  2.5558,  1.1593,  ...,  6.6224,  6.6642,  6.7467],\n        [14.8685,  0.3737, -0.0997,  ...,  8.0130,  8.1268,  7.4578],\n        [-0.1349,  0.9803, -0.0680,  ...,  6.1940,  6.3627,  6.5727]])"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.1610,  4.0913,  1.3561,  ...,  5.5406,  5.8819,  5.3047],\n        [ 2.9958,  0.6376,  0.2148,  ...,  6.6153,  5.6904,  6.6683],\n        [ 3.9623,  5.9557,  0.6369,  ...,  8.0138,  7.3404,  6.9348],\n        ...,\n        [15.3014,  2.5558,  1.1593,  ...,  6.6224,  6.6642,  6.7467],\n        [14.8685,  0.3737, -0.0997,  ...,  8.0130,  8.1268,  7.4578],\n        [-0.1349,  0.9803, -0.0680,  ...,  6.1940,  6.3627,  6.5727]])"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 864x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_loss[:])\n",
    "plt.title(\"train_loss\")\n",
    "plt.subplot(122)\n",
    "plt.plot(train_epochs_loss[1:],'-o',label=\"train_loss\")\n",
    "plt.plot(valid_epochs_loss[1:],'-o',label=\"valid_loss\")\n",
    "plt.plot(accuracy_list[1:],'-o',label=\"accuracy\")\n",
    "plt.title(\"epochs_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "from model.attention.SelfAttention import ScaledDotProductAttention\n",
    "import torch\n",
    "\n",
    "input=torch.randn(50,49,512)\n",
    "sa = ScaledDotProductAttention(d_model=16, d_k=140, d_v=140, h=8)\n",
    "output=sa(input,input,input)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}